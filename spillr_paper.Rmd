---
title: "`spillR`: Spillover Compensation in Mass Cytometry Data"
author: "Marco Guazzini$^{1}$, Alexander G. Reisach$^{2}$, Sebastian Weichwald$^{3}$, and Christof Seiler$^{1,4,5}$"
date: "$^1$Department of Advanced Computing Sciences, Maastricht University, The Netherlands \\\n $^2$Université Paris Cité, CNRS, MAP5, F-75006 Paris, France \\\n $^3$Department of Mathematical Sciences, University of Copenhagen, Denmark \\\n $^4$Mathematics Centre Maastricht, Maastricht University, The Netherlands \\\n $^5$Center of Experimental Rheumatology, Department of Rheumatology, \\\n University Hospital Zurich, University of Zurich, Switzerland \\\n \\\n `r gsub(' 0', ' ', format(Sys.time(), '%B %d, %Y'))`"
output:
  bookdown::pdf_document2: 
    toc: false
    extra_dependencies: ["euflag"]
bibliography: spillr_paper.bib
csl: style.csl
link-citations: true
abstract: |
  Channel interference in mass cytometry can cause spillover and may result in miscounting of protein markers. @catalyst introduce an experimental and computational procedure to estimate and compensate for spillover implemented in their R package `CATALYST`. They assume spillover can be described by a spillover matrix that encodes the ratio between unstained and stained channels. They estimate the spillover matrix from experiments with beads. We propose to skip the matrix estimation step and work directly with the full bead distributions. We develop a nonparametric finite mixture model, and use the mixture components to estimate the probability of spillover. Spillover correction is often a pre-processing step followed by downstream analyses, choosing a flexible model reduces the chance of introducing biases that can propagate downstream. We implement our method in an R package `spillR` using expectation-maximization to fit the mixture model. We test our method on synthetic and real data from `CATALYST`. We find that our method compensates low counts accurately, does not introduce negative counts, avoids overcompensating high counts, and preserves correlations between markers that may be biologically meaningful.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, echo=FALSE, warning=FALSE, message=FALSE}
library(CATALYST)
library(spillR)
library(nnls)
library(flowCore)
library(ggplot2)
library(tibble)
library(dplyr)
library(readr)
library(tidyr)
library(cowplot)
library(transport)
library(RColorBrewer)
library(spatstat.geom)
library(parallel)
library(kableExtra)
library(hexbin)
set.seed(23)
tfm <- function(x) asinh(x/5)
```

# Introduction

Mass cytometry makes it possible to count a large number of proteins simultaneously on individual cells [@bandura2009mass; @bendall2011single]. Although mass cytometry has less spillover---measurements from one channel overlap less with those of another---than flow cytometry [@sp-c; @novo2013generalized], spillover is still a problem and affects downstream analyses such as differential testing [@diffcyt; @seiler2021cytoglmm] or dimensionality reduction [@scater]. Reducing spillover by careful design of experiment is possible [@takahashi2017mass], but a purely experimental approach may be neither sufficient nor efficient [@lun2017influence].

@catalyst propose a method for addressing spillover by conducting an experiment on beads. This experiment measures spillover by staining each bead with a single type of antibody. The slope of the regression line between target antibody and non-target antibodies represents the spillover proportion between channels. @miao2021ab attempt to solve spillover by fitting a mixture model. Our contribution combines the solutions of @catalyst and @miao2021ab. We still require a bead experiment, as in @catalyst, but estimate spillover leveraging a statistical model, as in @miao2021ab. Both previous versions rely on an estimate for the spillover matrix. The spillover matrix encodes the pairwise spillover proportion between channels. We avoid estimating a spillover matrix and instead model spillover by fitting a mixture model to the observed counts. Our main new assumption is that the spillover distribution---not just the spillover proportion---from the bead experiment carries over to the biological experiment. In other words, we transfer the spillover distribution to the real experiment instead of just the spillover proportion encoded in the spillover matrix. The main difference between our method and the one by @catalyst is the average count after correcting for spillover. Our method will increase the average count after correction. By contrast, the method by @catalyst will shrink all the counts towards zero. As a consequence, the average count will be lower after correction.

In Section \@ref(methods), we present our mixture model and link it to calculating spillover probabilities for specific count values. Our estimation procedure is based on an EM algorithm and logistic regression, and implemented in our new R package `spillR`^[https://github.com/marcoguazzini/spillR]. In Section \@ref(results), we conduct experiments on simulated and real data obtained from the `CATALYST` R package [@catalyst]. Section \@ref(discussion) discusses our synthetic experiments and relates our findings to `CATALYST`.

\textcolor{red}{
Note on the terminology: In mass cytometry, counts are called dual counts as they are calculated from real pulse ion counts for lower values and converted from intensity values for higher values using the dual count coefficient. Some authors refer to dual counts as signal intensity. We will call them counts in this paper to illustrate the fact that we heavily rely on the fact that they are non-negative integers as opposed to possibly real valued intensities. 
}

# Methods

## Example

```{r method-example, fig.height=7, fig.width=7, out.width="75%", fig.align="center", fig.cap='A: Gaussian kernel density plot of target and spillover markers. B: The solid black curve represents the spillover probability estimated with our method with smoothing parameter $k = 11$. The grey curve is the estimate with $k = 3$. Larger values yield smoother spillover probability curves. C: Frequency polygons of marker CD3 (Yb173Di) comparing no correction, our method, and another method. Zero counts are not plotted. See Table \\ref{tab:method-example-summary} for the zero counts and means. Counts are arcsinh transformed with cofactor of five (\\protect\\hyperlink{ref-bendall2011single}{Bendall \\emph{et al.}, 2011}). The vertical dashed line helps to interpret the spillover correction when CD3 (Yb173Di) is $2.725$. This is an interesting point as it balances between spillover counts from Yb172Di measured in beads (panel A) and the counts measured on real cells (panel C). The spillover probability is around $0.5$. This means we will set about half of the counts to \\texttt{NA}\'s at this point. After correction the read curve (panel C) will be adjusted downwards to the green curve (panel C) with our method \\texttt{spillR}.', echo=FALSE, warning=FALSE, message=FALSE, cache = TRUE}
# constants
bc_key <- c(139, 141:156, 158:176)
ch <- "CD3.2"
ch_metal <- "Yb173Di"
ch_name <- "CD3 (Yb173Di)"
x_lim <- c(0, 7)

# --------- experiment with beads ---------

sce_bead <- prepData(ss_exp)
sce_bead <- assignPrelim(sce_bead, bc_key, verbose = FALSE)
sce_bead <- applyCutoffs(estCutoffs(sce_bead))
sce_bead <- computeSpillmat(sce_bead)

# --------- experiment with real cells ---------

data(mp_cells, package = "CATALYST")
sce <- prepData(mp_cells)

# --------- table for mapping markers and barcode ---------

marker_to_barc <- 
  rowData(sce_bead)[,c("channel_name", "is_bc")] |>
  as_tibble() |>
  dplyr::filter(is_bc == TRUE) |>
  mutate(barcode = bc_key) |>
  select(marker = channel_name, barcode)

# --------- call compensate from compCytof package ---------

sce_spillr <- compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE)

# --------- run CATALYST ---------

sm <- metadata(sce_bead)$spillover_matrix
sce_catalyst <- CATALYST::compCytof(sce, sm, overwrite = FALSE)

# --------- beads experiment ---------

tb_bead <- metadata(sce_spillr)$beads_distr[[ch_metal]]
tb_bead <- mutate(
  tb_bead, 
  barcode = ifelse(barcode == ch_metal, paste(ch_metal, "(target)"), barcode)
  )

p_beads <- tb_bead |>
  ggplot(aes(tfm(.data[[ch_metal]]), color = barcode)) +
  geom_density(adjust = 1, linewidth = 0.8) +
  xlim(x_lim) +
  xlab(ch_name) +
  ylab("density") + 
  ggtitle("Beads Experiment")

# --------- spillover probability curves ---------

tb_spill_prob <- metadata(sce_spillr)$spillover_est[[ch_metal]]

p_spill <- tb_spill_prob |>
  ggplot(aes(tfm(.data[[ch_metal]]), spill_prob)) +
  geom_line(linewidth = 0.8) +
  xlim(x_lim) +
  ylim(c(0, 1)) +
  xlab(ch_name) +
  ylab("probability") +
  ggtitle("Estimated Spillover")

# --------- before and after ---------

exprs_spillr <- sce_spillr |> 
  assay("exprs") |>
  t() |>
  as_tibble() |>
  mutate(correction = "none")
compexprs_spillr <- sce_spillr |> 
  assay("compexprs") |>
  t() |>
  as_tibble() |>
  mutate(correction = "spillR")
compexprs_catalyst <- sce_catalyst |> 
  assay("compexprs") |>
  t() |>
  as_tibble() |>
  mutate(correction = "CATALYST")
combo <- bind_rows(exprs_spillr, compexprs_spillr, compexprs_catalyst)
combo <- combo |> select(all_of(c(ch, "correction")))
combo$correction <- factor(combo$correction, 
                       levels = c("none", "spillR", "CATALYST"))

p_before_after <- combo |> 
  filter(.data[[ch]] > 0) |>
  ggplot(aes(.data[[ch]], color = correction, linetype = correction)) + 
  geom_freqpoly(alpha = 1.0, bins = 50, linewidth = 0.8) +
  xlim(x_lim) +
  xlab(ch_name) +
  ggtitle("Spillover Compensation on Real Cells")

# --------- combine everything ---------
v <- 2.725
plot_grid(
  p_beads + theme(legend.justification = c(0,1)) + 
    geom_vline(xintercept = v, linetype = "dashed"),
  p_spill + theme(legend.justification = c(0,1)) +
    geom_vline(xintercept = v, linetype = "dashed"), 
  p_before_after + theme(legend.justification = c(0,1)) +
    geom_vline(xintercept = v, linetype = "dashed"), 
  ncol = 1, align = "v", axis = "lr", labels = c("A", "B", "C")
)
```

Figure \@ref(fig:method-example) illustrates our procedure using a dataset from the `CATALYST` package as an example. There are four markers, HLA-DR (Yb171Di), HLA-ABC (Yb172Di), CD8 (Yb174Di), and CD45 (Yb176Di), that spill over into the target marker, CD3 (Yb173Di). The markers have two names: the first name is the protein name and the second name in brackets is the conjugated metal. There are bead experiments for each of the spillover markers.

Panel A depicts the marker distributions from the beads experiment. We see that for this marker the bead experiments are high-quality as the target marker Yb173Di is concentrated around six, similarly to the experiment with real cells. This suggests that the spillover marker values can be transferred to the real experiments. Marker Yb172Di shows large spillover into Yb173Di, and suggests that the left tail of the first mode of the distribution may be attributed to that marker. The other spillover markers have low counts, making it justifiable to set some or all the low counts to zero.

Panel B has a solid black and gray curves representing our spillover probability estimates. With the smoothing parameter $k = 11$, we can see that the probability of spillover goes up to around $0.5$. In that case, our correction step assigns around 50% of cells to spillover, and keeps the other 50% at the current value. We add a black dashed line to all plots at the position $2.725$ to illustrate this point. Low counts have spillover probability of one, which means that our procedure assigns them to spillover and masks them from the sample by setting them to `NA` values. Counts above four stem from spillover with probability zero (and from the actual target with probability one), which means that our procedure keeps them at their raw uncorrected value. The estimated curve with smoothing parameter $k = 3$ is more irregular, suggesting that these fluctuations may be driven by noise. Users can control the smoothing parameter $k$ to choose the desired bias-variance tradeoff.

```{r method-example-summary, echo=FALSE, warning=FALSE, message=FALSE}
zeros <- combo |>
  filter(.data[[ch]] == 0 | is.na(.data[[ch]])) |> 
  group_by(correction) |> 
  tally(name = "zeros or \\texttt{NA}'s")

means <- combo |> 
  filter(!is.na(.data[[ch]])) |> 
  group_by(correction) |>
  summarize(mean = mean(.data[[ch]]))

zeros_means <- left_join(zeros, means, by = "correction")
levels(zeros_means$correction)[2] <- "\\texttt{spillR}"
levels(zeros_means$correction)[3] <- "\\texttt{CATALYST}"

zeros_means |>
  kableExtra::kbl(
    caption = "Additional summaries of the data underlying Figure \\ref{fig:method-example}C.", 
    booktabs = TRUE, digits = 2, escape = FALSE
    )
```

Panel C displays the distribution of our target marker, CD3 (Yb173Di), before and after spillover correction. Our compensation method, `spillR`, masks most markers in the low counts up to two, about 25% to 50% in the medium counts range between two and four, and keeps counts above four. Larger counts are not affected by the correction, as can be seen by the overlap of both curves. In contrast, the other method, `CATALYST`, shifts large counts to the left, and shifts medium counts to low counts or zero counts. Zero counts (or equivalently `NA` counts for `spillR`) and mean counts are shown in Table \@ref(tab:method-example-summary). 
\textcolor{red}{
As we do not know the true mean in this case, the comparison of means here serves merely as an illustration of the main differences between \texttt{spillR} and \texttt{CATALYST} and not to identify the correct method.
}

## Definition of Spillover Probability and Assumptions

We observe a count $Y_i$ of a target marker in cell $i$. We model the observed $Y_i$ as a finite mixture [@mclachlan2019finite] of unobserved true marker counts $Y_i \mid Z_i = 1$ and spillover marker counts $Y_i \mid Z_i = 2, \dots, Y_i \mid Z_i = K$ with mixing probabilities $\pi_{k} = P(Z_i = k)$ for $k = 1, \dots, K$, 
$$
P(Y_i = y) = \sum_{k = 1}^K \pi_k \, P(Y_i = y \mid Z_i = k).
$$
The first mixing probability is the proportion of true signal in the observed counts. The other $K-1$ mixing probabilities are the proportions of spillover. The total sum of mixing probabilities equals one, $\sum_k \pi_k = 1$. The total number of markers in mass cytometry panels is between 30 and 40 [@bendall2011single], but only a small subset of three to four markers spill over into the target marker [@catalyst]. So, typically $K = 1+3$ or $K = 1+4$.

Experimentally, we only measure a sample from the distribution of $Y_i$. The probabilities $\pi_k$ and true distributions $P(Y_i = y \mid Z_i = k)$ are unobserved, and we need to estimate them from data. In many applications, the mixture components are in a parametric family, for example, the negative binomial distribution. As spillover correction is a pre-processing step followed by downstream analyses, choosing the wrong model can introduce biases in the next analysis step. To mitigate such biases, we propose to fit nonparametric mixture components. We make two assumptions that render the components and mixture probabilities identifiable:

* <div id='assumption1'>(A1) Spillover distributions are the same in bead and real experiments.</div> 
The distribution of $Y_i \mid Z_i = k$ for all $k > 1$ is the same in beads and real cells. This assumption allows us to learn the spillover distributions of $Y_i \mid Z_i = k$ for all $k > 1$ from experiments with beads, and transfer them to the experiment with real cells. This assumption relies on high-quality single stained bead experiments that measure spillover in the same range as the target biological experiment. In other words, a high-quality bead experiment for our method works best if the distribution of bead cells is similar to the distribution of real cells.

* <div id='assumption2'>(A2) For each cell $i$, the observed count $Y_i$ can only be due to one distribution.</div> 
This assumption is already implied by the statement of the mixture model. It allows us to calculate the spillover probability for a given count $Y_i = y$ from the posterior probability that it arises through spillover from markers $k > 1$,
$$
P(\text{spillover} \mid Y_i = y) = P(Z_i > 1 \mid Y_i = y) = 
1 - P(Z_i = 1 \mid Y_i = y) = 
1 - \frac{\pi_1 \, P(Y_i = y \mid Z_i = 1)}{P(Y_i = y)}.
$$
To parse this calculation, recall that in mixture models the $\pi_1$ is the prior probability, $P(Y_i = y \mid Z_i = 1)$ is the conditional probability given the mixture component, and the denominator $P(Y_i = y)$ is the marginal distribution. Applying Bayes rule leads to the posterior probability.

## Estimation of Spillover Probability

We propose a two step procedure for estimating the spillover probability. In step 1, we estimate mixture components and mixture probabilities. We refine these estimates using the EM algorithm [@dempster1977maximum]. In step 2, we use these probability estimates to assign counts to spillover or signal.

We denote the $n \times K$ count matrix as $\mathbf{Y} = (y_{ik})$ with real cells in the first column and beads in columns two and higher. To simplify mathematical notation but without loss of generality, we assume that the number of 
\textcolor{red}{events}
from real and bead experiments have the same $n$. In practice, the number of 
\textcolor{red}{events}
from bead experiments is much smaller than from real experiments. The $k$th column of $\mathbf{Y}$ contains marker counts for the $k$th spillover marker, which represents the empirical spillover distribution of marker $k$ into the target marker, that is, the marker in the first column of $\mathbf{Y}$.

We use the empirical weighted cumulative distribution function (CDF) with non-negative weights $w_i$ at each data point to model the marker distributions $F$ for a marker $k$,
$$
\widehat{F}_k(y) = \frac{\sum_{i = 1}^n w_i \, \mathbf{1} \left( y_{ik} \le y \right)}{\sum_{i = 1}^n w_i},
$$
where $\mathbf{1}(\cdot)$ is the indicator function.

### EM Algorithm

* Initialization: For the mixture probability vector, we assign probability $0.9$ to the the target marker and divide the probability $0.1$ among the spillover markers, 
$$
\hat{\pi}_{1} = 0.9 \text{ and } \hat{\pi}_i = 0.1/(K-1) \text{ for all } i > 1,
$$ 
and evaluate the $k$th mixture component using its empirical CDF with equal weights for all data points,
$$
\widehat{P}(Y_i = y \mid Z_i = k) = \widehat{F}_k(y) - \widehat{F}_k(y-1) \text{ with } w_i = 1/n.
$$
We smooth the probability mass function (PMF), $\widehat{P}(Y_i = y \mid Z_i = k)$, using running medians with a fixed window size as implemented in R function `runmed`. The procedure is not sensitive to the choice of the initial mixture probability vector. Other settings are possible, e.g., setting all probabilities to the same value.

* E-step: We evaluate the posterior probability of a count $y$ belonging to component $k$ (that is, originating from marker $k$),
$$
\widehat{P}\left(Z_i = k \mid Y_i = y \right) = 
\frac
{ \hat{\pi}_k \, \widehat{P}(Y_i = y \mid Z_i = k) }
{ \sum_{k' = 1}^K \hat{\pi}_{k'} \, \widehat{P}(Y_i = y \mid Z_i = k') }.
$$

* M-step: We estimate the new mixture probability vector from posterior probabilities, 
$$
\hat{\pi}_k = 
\frac{1}{n} \sum_{i = 1}^n \widehat{P}\left(Z_i = k \mid Y_i = y \right),
$$
and estimate the new target marker distribution using its empirical CDF with weights set to posterior probabilities,
$$
\widehat{P}(Y_i = y \mid Z_i = 1) = \widehat{F}_1(y) - \widehat{F}_1(y-1) \text{ with } w_i = \widehat{P} \left(Z_i = 1 \mid Y_i = y \right).
$$
As before, we smooth the PMF, $\widehat{P}(Y_i = y \mid Z_i = 1)$, using running medians with the same window size. We keep the bead distributions, $\widehat{P}(Y_i = y \mid Z_i = k)$ for all $k > 1$, fixed at their initial value.

To refine our estimates, we iterate over the E and M-steps until estimates stabilize. We stop iterating when $\hat{\pi}_1$ changes less than $10^{-5}$ from the previous iteration. The final output is the spillover probability curve with estimates at discrete points in the support of $Y_i$, 
$$
\widehat{P}(\text{spillover} \mid Y_i = y) = 1 - \widehat{P}(Z_i = 1 \mid Y_i = y).
$$

We rely on assumption [(A1)](#assumption1) to justify updating only the distribution of the target marker. We rely on assumption [(A2)](#assumption2) to justify calculating the spillover probability from the mixture model. We refer to Appendix \@ref(em-algorithm-example) for a step-by-step example of our EM algorithm. 

### Spillover Decision

To perform the spillover compensation, we draw from a Bernoulli distribution with the spillover probability as parameter to decide whether or not to assign a given count to spillover. We consider spillover counts as having no clear biological interpretation and mask them from our dataset while keeping all other counts. In our implementation, we choose to set spillover counts to `NA` instead of zero to avoid zero-inflated distributions. 

<!-- 

## Identifiability

[TODO] Need to think if our model assumptions guarantee identifiability and if our procedure is guaranteed to converge. I think if it is identifiable, then convergence follows from the property of the EM algorithm. Some related work [@hall2003nonparametric; @aragam2020identifiability] that might help. 

-->

# Results

We first evaluate our new method `spillR` on simulated datasets. We probe our method to experimentally find its shortcomings. Then, we compare `spillR` to the non-negative least squares method implemented in the R package `CATALYST` on real data from the same package. All experiments and plots can be reproduced by compiling the R markdown file `spillR_paper.Rmd`^[https://github.com/ChristofSeiler/spillR_paper].

## Simulated Data

```{r simulated-experiments, echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE}
# --------- global parameters ---------

n_real <- 10000
n_bead <-  1000
lambda_real <- 200
lambda_bead  <- 70
lambda_bead_high  <- 330
spill_prob <- 0.5
n_rep <- 20
n_cores <- 16

# --------- helper functions ---------

compute_average <- function(data) {
  comp <- spillR:::compensate(data$df_real, data$df_bead, 
                              target_marker = "Y", 
                              spillover_markers = "Z")
  y_comp  <- comp$tb_compensate$corrected
  y_truth <- data$Z_target

  tibble(
    mean_obsv = mean(data$df_real$Y),
    mean_comp = mean(y_comp, na.rm = TRUE),
    mean_truth = mean(y_truth)
  )
}

# --------- panel A ---------

generate_data_a <- function(tau) {
  # real experiment
  I    <- rbinom(n = n_real, size = 1, prob = spill_prob)
  Z_target <- rpois(n = n_real, lambda = lambda_real)
  Z_spill  <- rpois(n = n_real, lambda = lambda_bead+tau)
  Y        <- (1-I)*Z_target + I*Z_spill
  df_real  <- tibble(Y = Y, barcode = "Y", type = "real cells")
  
  # bead experiment
  Z_bead   <- rpois(n = n_bead, lambda = lambda_bead)
  df_bead  <- tibble(Y = Z_bead, barcode = "Z", type = "beads")
  
  list(df_real = df_real, df_bead = df_bead, 
       Z_target = Z_target, Z_spill = Z_spill)
}

# create parameter combination
d_a <- expand.grid(
  tau = seq(-10, 0, length.out = 60), 
  replicate = 1:n_rep,
  title = "Bead Shift"
  )
d_averages <- mclapply(d_a$tau, function(tau) {
  data <- generate_data_a(tau)
  compute_average(data)
}, mc.cores = n_cores) |> bind_rows()
d_a <- bind_cols(d_a, d_averages)

# --------- panel B ---------

generate_data_b <- function(tau) {
  # real experiment
  I <- rbinom(n = n_real, size = 1, prob = spill_prob)
  T <- rpois(n = n_real, lambda = lambda_real)
  S <- rpois(n = n_real, lambda = lambda_bead)
  M <- rbinom(n = n_real, size = 1, prob = tau)
  Z_target <- (1-M)*T + M*S
  Z_spill  <- (1-M)*S + M*T
  Y        <- (1-I)*Z_target + I*Z_spill
  df_real  <- tibble(Y = Y,
                     barcode = "Y",
                     type = "real cells")
  
  # bead experiment
  I <- rbinom(n = n_bead, size = 1, prob = spill_prob)
  T <- rpois(n = n_bead, lambda = lambda_real)
  S <- rpois(n = n_bead, lambda = lambda_bead)
  M <- rbinom(n = n_bead, size = 1, prob = tau)
  Z_bead <- (1-M)*S + M*T
  df_bead  <- tibble(Y = Z_bead,
                     barcode = "Z",
                     type = "beads")
  
  list(df_real = df_real, df_bead = df_bead, 
       Z_target = Z_target, Z_spill = Z_spill)
}

# create parameter combination
d_b <- expand.grid(
  tau = seq(from = 0, to = 0.5, length.out = 60), 
  replicate = 1:n_rep,
  title = "Model Misspecification"
  )
d_averages <- mclapply(d_b$tau, function(tau) {
  data <- generate_data_b(tau)
  compute_average(data)
}, mc.cores = n_cores) |> bind_rows()
d_b <- bind_cols(d_b, d_averages)

# --------- panel C ---------

generate_data_c <- function(tau) {
  # real experiment
  I        <- rbinom(n = n_real, size = 1, prob = spill_prob)
  Z_target <- rpois(n = n_real, lambda = lambda_real)
  H        <- rbinom(n = n_real, size = 1, prob = tau)
  Z_spill1 <- rpois(n = n_real, lambda = lambda_bead)
  Z_spill2 <- rpois(n = n_real, lambda = lambda_bead_high)
  Z_spill   <- (1-H)*Z_spill1 + H*Z_spill2
  Y         <- (1-I)*Z_target + I*Z_spill
  df_real   <- tibble(Y = Y, barcode = "Y", type = "real cells")
  
  # bead experiment
  H       <- rbinom(n = n_real, size = 1, prob = tau)
  Z_bead1 <- rpois(n = n_real, lambda = lambda_bead)
  Z_bead2 <- rpois(n = n_real, lambda = lambda_bead_high)
  Z_bead  <- (1-H)*Z_bead1 + H*Z_bead2
  df_bead <- tibble(Y = Z_bead, barcode = "Z", type = "beads")

  list(df_real = df_real, df_bead = df_bead, 
       Z_target = Z_target, Z_spill = Z_spill)
}

# create parameter combination
d_c <- expand.grid(
  tau = seq(from = 0, to = 1, length.out = 60), 
  replicate = 1:n_rep,
  title = "Bimodal Spillover"
  ) 
d_averages <- mclapply(d_c$tau, function(tau) {
  data <- generate_data_c(tau)
  compute_average(data)
}, mc.cores = n_cores) |> bind_rows()
d_c <- bind_cols(d_c, d_averages)

# --------- combined data frame ---------

tb_experiment <- bind_rows(d_a, d_b, d_c)
tb_summary <- tb_experiment |> 
    group_by(title, tau) |> 
    summarize(
      `mean(Y)` = mean(mean_obsv),
      `mean(Y | Z = 1)` = mean(mean_truth),
      `spillR mean(Y)` = mean(mean_comp)
      ) |> 
    ungroup()

tb_summary_long <- pivot_longer(
  tb_summary, -c(title, tau), names_to = "mean", values_to = "count"
  ) |>
  mutate(mean = factor(mean, levels = c("mean(Y)",
                                        "mean(Y | Z = 1)", 
                                        "spillR mean(Y)")))
```

```{r simulated-experiments-critical-values, echo = FALSE, warning = FALSE, message = FALSE}
# --------- helper function ---------
plot_critical <- function(experiment, taus, simulated, reverse = FALSE) {
  
  var_names <- c(
    "Y",
    "Y | Z = 1",
    "Y | Z = 2"
    )
  
  simulated <- lapply(simulated, function(data) {
    bind_rows(
      tibble(count = data$df_real$Y, variable = var_names[1]),
      tibble(count = data$Z_target,  variable = var_names[2]),
      tibble(count = data$df_bead$Y, variable = var_names[3])
    )})
  
  df_simulated <- lapply(
    seq(taus), function(i) mutate(simulated[[i]], tau = taus[i])
    ) |> 
    bind_rows() |>
    mutate(variable = factor(variable, levels = var_names)) |>
    mutate(tau = factor(tau, levels = taus))
  
  g_overview <- tb_summary_long |> 
    filter(title == experiment) |>
    ggplot(aes(tau, count, color = mean, linetype = mean)) +
    geom_line() + 
    scale_color_manual(values = c("#E69F00", "#000000", "#009E73")) +
    scale_linetype_manual(values = c("solid", "dashed", "solid")) +
    ylab("mean")
  if(reverse) g_overview <- g_overview + scale_x_reverse()
  
  g_critical <- df_simulated |>
    ggplot(aes(count, color = variable, linetype = variable)) + 
    geom_density(key_glyph = "path") +
    facet_wrap(~tau, labeller = label_both) +
    scale_color_manual(values = c("#E69F00", "#000000", "#56B4E9")) +
    scale_linetype_manual(values = c("solid", "dashed", "solid"))
  
  plot_grid(
    ggdraw() + 
      draw_label(experiment, x = 0, hjust = 0) + 
      theme(plot.margin = margin(0, 0, 0, 42)), 
    plot_grid(g_overview, g_critical, 
              ncol = 1, rel_heights = c(0.45, 0.55), align = "v", axis = "lr"),
    ncol = 1, rel_heights = c(0.1, 1)
    )
}

# --------- A ---------

experiment <- "Bead Shift"
taus <- c(0, -5, -10)
simulated <- lapply(taus, generate_data_a)
p_a <- plot_critical(experiment, taus, simulated, reverse = TRUE)

# --------- B ---------

experiment <- "Model Misspecification"
taus <- c(0, 0.25, 0.5)
simulated <- lapply(taus, generate_data_b)
p_b <- plot_critical(experiment, taus, simulated)

# --------- C ---------
experiment <- "Bimodal Spillover"
taus <- c(0, 0.5, 1.0)
simulated <- lapply(taus, generate_data_c)
p_c <- plot_critical(experiment, taus, simulated)
```

```{r simulated-experiments-plot, fig.height = 12, fig.width = 9, out.width = "90%", fig.align = "center", echo = FALSE, warning = FALSE, message = FALSE, fig.cap="Three experiments testing our assumptions and sensitivity to bimodal bead distribution. For each experiment the top row are mean values over the entire range of the experimental setups, and the bottom row are density plots for three parameter settings to illustrate the generated distributions. $Y$ is the distribution with spillover. $Y \\mid Z = 1$ is the distribution without spillover. $Y \\mid Z = 2$ is the spillover. mean($Y$) is the average of the distribution with spillover. mean($Y \\mid Z = 1$) is the average count without spillover. \\texttt{spillR} mean($Y$) is the average count after correcting $Y$."}
plot_grid(p_a, p_b, p_c, labels = c("A", "B", "C"),
          ncol = 1, align = "v", axis = "lr")
```

We choose three different experiments to test `spillR` against different bead and real cell distributions. We explore a wide range of possible parameter settings. Figure \@ref(fig:simulated-experiments-plot) has three panels, each representing one experimental setup. The first two panels test our assumptions [(A1)](#assumption1) and [(A2)](#assumption2). The third panel tests sensitivity of `spillR` to bimodal bead distributions. For all three experiments, we model counts using a Poisson distribution with parameter $\lambda$. We simulate 10,000 real cells with $\lambda = 200$, and 1,000 beads with $\lambda = 70$, and spillover probability of $0.5$. Beads are an independent copy of the true spillover. The other parameters and statistical dependencies are specific to each experiment. The details of the generative models are given in Appendix \@ref(generative-models). We repeat each simulation 20 times and report averages over the 20 replications. Each panel of Figure \@ref(fig:simulated-experiments-plot) has two rows of plots. The plot on the first row represents the summary of the means for each experimental setup as a function of their respective parameter $\tau$. This parameter has a different meaning in each setup. To visualize the different experiments, we summarize the full distributions with the true simulated signal mean (black), the uncorrected mean (orange), and the `spillR` corrected mean (green). Plots on the second row illustrate the simulated data distributions for three selected parameters $\tau$ picked from the experimental setup. The yellow density curve is the observed count $Y$. The black density curve is the target cell count. The blue density curve is the spillover distribution. The goal of the experiment is to estimate the mean of the black density as accurately as possible from the yellow density curve, which represents the data $Y$ that we would observe in practice. We simulate this data ourselves with the models in Appendix \@ref(generative-models).

<!--
### Bead Shift (A1)
-->

In the first experiment (panel A), we shift the measured beads spillover away from the true bead spillover to probe [(A1)](#assumption1). We test a wide range of bead shifts from no shift at $\tau = 0$ to $\tau = -30$. At $\tau = -30$, the measured spillover (the first mode of the yellow density) is shifted away from the actual spillover (the blue density). Such low-quality beads cause both the observed and compensated mean to be below the true mean. As the beads quality improves, the compensated signal moves closer to the true mean. As we increase $\tau$ the first mode of the yellow density moves towards the blue density. This represents high-quality bead experiments. In all cases, even for bad beads experiments, our compensation improves the means. Our compensation also increases means, as it should, in contrast to e.g. @catalyst.

<!--
### Model Misspecification (A2)
-->

In the second experiment (panel B), we mixed target and spillover to explore the robustness of our method with respect to our second assumption [(A2)](#assumption2). One way to think about this is that the mixture is a form of model misspecification. Our mixture model is undercomplete, which means that there are more true mixture components than we observe in the beads experiment. If $\tau = 0$, then assumption [(A2)](#assumption2) is correct, but for $\tau = 0.5$ the assumption [(A2)](#assumption2) is maximally violated. The true mean decrease with increasing $\tau$. Our compensation is closer to the true mean across the tested range. At $\tau = 0.5$ all three distributions and their means are the same.

<!--
### Bimodal Spillover
-->

In the third experiment (panel C), we model spillover with a bimodal distribution. Here $\tau$ is the mixing probability of the two modes. The locations of the two spillover modes are fixed. If $\tau = 0$ or $\tau = 1$, then spillover is unimodal. If $\tau = 0.5$, the first mode of the bimodal beads distribution is left to the signal mode, and the second mode is to the right. The corrected mean is closer to the true mean than the uncorrected mean across the test range.

## Real Data

```{r spillr-vignette, fig.height = 6, fig.width = 10, out.width = "100%", fig.align = "center", echo = FALSE, warning = FALSE, message = FALSE, fig.cap="Comparison of compensation methods and uncorrected counts on real data. Counts are arcsinh transformed with cofactor of five (\\protect\\hyperlink{ref-bendall2011single}{Bendall \\emph{et al.}, 2011}).", cache = TRUE}
# constants
bc_key <- c(139, 141:156, 158:176)

# helper functions
colorscale = scale_fill_gradientn(
  colors = rev(brewer.pal(9, "YlGnBu")),
  values = c(0, exp(seq(-5, 0, length.out = 100)))
  )

# --------- experiment with beads ---------

sce_bead <- prepData(ss_exp)
sce_bead <- assignPrelim(sce_bead, bc_key, verbose = FALSE)
sce_bead <- applyCutoffs(estCutoffs(sce_bead))
sce_bead <- computeSpillmat(sce_bead)

# --------- experiment with real cells ---------

data(mp_cells, package = "CATALYST")
sce <- prepData(mp_cells)

# --------- table for mapping markers and barcode ---------
marker_to_barc <- 
  rowData(sce_bead)[,c("channel_name", "is_bc")] |>
  as_tibble() |>
  dplyr::filter(is_bc == TRUE) |>
  mutate(barcode = bc_key) |>
  dplyr::select(marker = channel_name, barcode)

# --------- call compensate from compCytof package ---------
sce_spillr <- compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE)

# --------- 2d histogram from spillR (for vignette, not for paper) ---------
# as <- c("counts", "exprs", "compcounts", "compexprs")
# chs <- c( "Yb171Di", "Yb173Di")
# ps <- lapply(as, function(a) 
#     plotScatter(sce_spillr, chs, assay = a))
# plot_grid(plotlist = ps, nrow = 2)

# --------- run CATALYST ---------
sm <- metadata(sce_bead)$spillover_matrix
sce_catalyst <- CATALYST::compCytof(sce, sm, overwrite = FALSE)

# --------- compare spillR and CATALYST (Figure 3B) ---------
exprs_spillr <- sce_spillr |> 
  assay("exprs") |>
  t() |>
  as_tibble() |>
  mutate(method = "uncorrected")
compexprs_spillr <- sce_spillr |> 
  assay("compexprs") |>
  t() |>
  as_tibble() |>
  mutate(method = "spillR")
compexprs_catalyst <- sce_catalyst |> 
  assay("compexprs") |>
  t() |>
  as_tibble() |>
  mutate(method = "CATALYST")
combo <- bind_rows(exprs_spillr, compexprs_spillr, compexprs_catalyst)
combo$method <- factor(combo$method, levels=c('uncorrected', 'CATALYST', 'spillR'))

# row 1
p1 <- ggplot(combo, aes(x = CD3.2, y = CD8b)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("CD3 (Yb173Di)") + ylab("CD8 (Yb174Di)")
p2 <- ggplot(combo, aes(x = CD3.2, y = CD8)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("CD3 (Yb173Di)") + ylab("CD8 (La139Di)")

# row 2
p3 <- ggplot(combo, aes(x = HLA.ABC, y = CD3.2)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("CD3 (Yb173Di)")
p4 <- ggplot(combo, aes(x = HLA.ABC, y = CD3.1)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("CD3 (Sm147Di)")

# row 3
p5 <- ggplot(combo, aes(x = HLA.ABC, y = HLA.DR.1)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("HLA-DR (Yb171Di)")
p6 <- ggplot(combo, aes(x = HLA.ABC, y = HLA.DR.2)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("HLA-DR (Lu175Di)")

plot_grid(p1, p2, p3, p4, p5, p6, ncol = 2, 
          labels = c("A", "B", "C", "D", "E", "F"))
```

We compare our method to `CATALYST` on one of the example datasets in the `CATALYST` package. The dataset has an experiment with real cells and a corresponding bead experiment. The experiment on real cells has 5,000 peripheral blood mononuclear cells from healthy donors measured on 39 channels. The experiment on beads has 10,000 cells measured on 36 channels. They have single stained bead experiments. The number of beads per metal label range from 112 to 241.

We compare the two methods on the same markers as in the original `CATALYST` paper [@catalyst] in their Figure 3B. In the original experiment, they conjugated three proteins---CD3, CD8, and HLA-DR---with two different metal labels. They conjugated CD8 (first row in Figure \@ref(fig:spillr-vignette)) with Yb174Di (Yb is the metal and 174 is the number of 
\textcolor{red}{nucleons}
of the isotope) and La139Di, and similarly for the other rows. On the horizontal axis, we plot the same markers as in the original paper, CD3 and HLA-ABC. We visualize the joint distributions using two-dimensional histograms.

In all six panels (A--F), we observe that `spillR` compensates most strongly in the low counts. In panel C, CD3 (Yb173Di) against HLA-ABC (Yb172Di), `CATALYST` can be seen to compensate strongly in the middle range. It removes the spherical pattern that shows correlation between the two markers. `spillR` preserves this correlation structure and only masks out the lower counts of CD3 (Yb173Di). This highlights a key difference between `spillR` and `CATALYST`: `spillR` does not correct all counts by shrinking them, but rather removes some counts, following the idea that the distribution of the remaining counts is close to the true distribution. `CATALYST` follows another strategy by shrinking counts across the entire range.

\textcolor{red}{
The panels D and F no compensation from \texttt{CATALYST} and similar compensation to panels C and E from \texttt{spillR}. We checked the diagnostic plots using function \texttt{plotDiagnostics} in \texttt{spillR} to investigate the reason for the strong compensation. The design of the original experiment is such that we expect no spillover in the channels on the vertical axis. But our diagnostic plots show that there the bead distribution from spillover marker as identified by the spillover matrix in \texttt{CATALYST} exhibit strong overlap between at least one spillover marker then the first mode in the distribution of real cells. Thus our strong compensation is consistent with our assumption (A1).
} 
<!-- [(A1)](#assumption1) -->

The color code of the two-dimensional histograms indicates the absolute number of cells that fall into one hexagon bin. The uncorrected and `spillR` corrected histograms can contain different absolute numbers of cells
\textcolor{red}{
even for identical distributions. The reason for that is that \texttt{spillR} rounds counts to integers and converts spillover counts to \texttt{NA} values. We perform a rounding step to convert raw mass cytometry data that is often not count data to the next lower integer. This often happens because proprietary post-processing of the manufacturer perform a randomization step when exporting the data.
}
The uncorrected counts do not undergo this pre-processing step. `CATALYST` does not perform this pre-processing step. This also explains the different patterns in panel B. `spillR` has horizontal stripes that correspond to non-integer values not in the support of the distribution for `spillR`.
\textcolor{red}{
We leave the decision to apply re-randomization of the count data for downstream analysis up to the user.
}

\textcolor{red}{
The computational time and memory consumption are for \texttt{spillR} are: around 30 seconds with less than 1 GB of RAM. In comparison, for \texttt{CATALYST} is less than one second with less than 1GB of RAM..
}

## Semi-Simulated Data

```{r semi-simulated-plot, fig.height = 5, fig.width = 10, out.width = "100%", fig.align = "center", echo = FALSE, warning = FALSE, message = FALSE, fig.cap="Comparison of compensation methods and uncorrected counts on semi-synthetic data. The vertical dashed line helps to interpret the spillover correction when CD3 (Yb173Di) is 2.725. Counts are arcsinh transformed with cofactor of five (\\protect\\hyperlink{ref-bendall2011single}{Bendall \\emph{et al.}, 2011}).", cache = TRUE}
# --------- helper function ---------
inv_tfm <- function(x) 5*sinh(x)

# --------- load experiment data of real cells and beads ---------
sce <- prepData(mp_cells)

semi_synthetic_data <- function(shift) {

  # --------- modify bead data ---------
  sce_bead <- prepData(ss_exp)
  
  sce_bead <- assignPrelim(sce_bead, bc_key, verbose = FALSE)
  sce_bead <- applyCutoffs(estCutoffs(sce_bead))
  sce_bead <- computeSpillmat(sce_bead)
  
  counts_bead <- assay(sce_bead, "counts")
  target_channel <- "Yb173Di"
  target_barcode <- "172"
  target_marker <- rowData(sce_bead) |> 
    as_tibble() |>
    filter(channel_name == target_channel) |> 
    pull(marker_name)
  col_ids <- which(sce_bead$bc_id == target_barcode)
  
  counts_real <- assay(sce, "counts")
  y_real <- counts_real[target_marker, ]
  y_bead <- y_real[y_real > 10 & y_real < 300]
  y_bead_shifted <- inv_tfm(tfm(y_bead) - shift)
  y_bead_shifted <- pmax(y_bead_shifted, 0)
  y_bead_small <- sample(y_bead_shifted, length(col_ids))
  counts_bead[target_marker, col_ids] <- y_bead_small
  assay(sce_bead, "counts") <- counts_bead
  
  sce_bead <- assignPrelim(sce_bead, bc_key, verbose = FALSE)
  sce_bead <- applyCutoffs(estCutoffs(sce_bead))
  sce_bead <- computeSpillmat(sce_bead)
  
  # --------- CATALYST ---------
  sm <- metadata(sce_bead)$spillover_matrix
  sce_catalyst <- CATALYST::compCytof(sce, sm, method = "nnls", overwrite = FALSE)
  
  # --------- spillR ---------
  marker_to_barc <- 
    rowData(sce_bead)[,c("channel_name", "is_bc")] |>
    as_tibble() |>
    dplyr::filter(is_bc == TRUE) |>
    mutate(barcode = bc_key) |>
    dplyr::select(marker = channel_name, barcode)
  sce_spillr <- compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE)
  
  # --------- beads experiment ---------
  tb_bead <- metadata(sce_spillr)$beads_distr[[ch_metal]]
  tb_bead <- mutate(
    tb_bead, 
    barcode = ifelse(barcode == target_channel, paste(target_channel, "(target)"), barcode)
    )
  tb_bead <- mutate(tb_bead, shift = shift)
  
  # --------- before and after ---------
  exprs_spillr <- sce_spillr |> 
    assay("exprs") |>
    t() |>
    as_tibble() |>
    mutate(correction = "none")
  compexprs_spillr <- sce_spillr |> 
    assay("compexprs") |>
    t() |>
    as_tibble() |>
    mutate(correction = "spillR")
  compexprs_catalyst <- sce_catalyst |> 
    assay("compexprs") |>
    t() |>
    as_tibble() |>
    mutate(correction = "CATALYST")
  combo <- bind_rows(exprs_spillr, compexprs_spillr, compexprs_catalyst)
  combo <- combo |> select(all_of(c(ch, "correction")))
  combo$correction <- factor(combo$correction, 
                         levels = c("none", "spillR", "CATALYST"))
  combo <- mutate(combo, shift = shift)
  
  list(tb_bead = tb_bead, combo = combo)

}

semi_list <- lapply(c(0, 0.47, 0.94), semi_synthetic_data)
tb_bead <- lapply(semi_list, function(x) x$tb_bead) |> bind_rows()
combo <- lapply(semi_list, function(x) x$combo) |> bind_rows()

p_beads <- tb_bead |>
  ggplot(aes(tfm(.data[[ch_metal]]), color = barcode)) +
  geom_density(adjust = 1, linewidth = 0.8) +
  xlim(x_lim) +
  xlab(ch_name) +
  ylab("density") + 
  ggtitle("Beads Experiment") +
  facet_wrap(~shift, labeller = label_both)

p_before_after <- combo |> 
  filter(.data[[ch]] > 0) |>
  ggplot(aes(.data[[ch]], color = correction, linetype = correction)) + 
  geom_freqpoly(alpha = 1.0, bins = 50, linewidth = 0.8) +
  xlim(x_lim) +
  xlab(ch_name) +
  ggtitle("Spillover Compensation on Real Cells") +
  facet_wrap(~shift, labeller = label_both)

# --------- combine everything ---------

v <- 2.725
plot_grid(
  p_beads + theme(legend.justification = c(0,1)) + 
    geom_vline(xintercept = v, linetype = "dashed"),
  p_before_after + theme(legend.justification = c(0,1)) +
    geom_vline(xintercept = v, linetype = "dashed"), 
  ncol = 1, align = "v", axis = "lr", labels = c("A", "B")
)
```

We compare `spillR` and `CATALYST` on semi-simulated data. The goal is to elucidate the main differences between `spillR` and `CATALYST`, and to evaluate the performance of `spillR` on more than one spillover marker. We create semi-simulated datasets by overwriting the beads distribution for CD3 (Yb173Di). We take the first mode of the distribution of CD3 (Yb173Di) in real cells by filtering counts that range from 10 to 300. We shift these filtered counts by three different values: no shift is 0, subtracting 0.47 on the transformed scale, and subtracting 0.94 on the transformed scale. We further sample without replacement from this new bead distribution to keep the same number of beads as in the original dataset. Figure \@ref(fig:semi-simulated-plot) shows the three different datasets. In panel A, all bead distribution are equal to the original dataset in Figure \@ref(fig:method-example). Only the bead with barcode Yb173Di shift by 0.47 and 0.94 in the middle and right most plot. Both methdos correctly compensate the spillover mode when no shift is present. `CATALYST` compensate more aggressively with the medium shift of 0.47, and `spillR` is more moderate and compensate only the left hand tail of the spillover mode. In the largest shift of 0.94, the two methods behave differently. `CATALYST` shrink counts towards zero, shifting the entire spillover mode towards zero. In contrast, `spillR` just compensates lightly on the left hand tail.

# Discussion

The experiment for [(A1)](#assumption1) shows that the mean count after `spillR` correction is closer to the true mean over a wide range of bead shifts. This indicates that our method can perform well even if the bead experiments are imperfect. If the difference between distributions of beads and real cells is large, then one option is to rerun the bead experiments to reduce this gap. The experiment for [(A2)](#assumption2) shows that our method is also robust to model misspecification. Additionally, misspecification can be addressed by adding all channels if necessary. The increase in computational cost when adding channels is relatively minor as our method scales linearly in the number of spillover markers. The experiment on bimodal bead distributions shows that the mean count after correction is still closer to the true mean even with bimodal bead distributions, and also if the spillover is actually larger than the true signal.

In our comparison with `CATALYST` on real data, we observe the effect of the two different correction strategies. `CATALYST` essentially shrinks counts towards zero by minimizing a non-negative least squares objective. It assumes that spillover is linear up to counts of 5,000. The applied shrinkage is the same for low counts (e.g., below 10) and high counts (e.g., more than 100). By contrast, `spillR` does not require linearity of the spillover, but assumes that the distribution on the beads experiment carries over to the real cells experiment. In other words, the optimal beads experiment has the same peaks as the real cells experiment.

If counts are in the spillover range (which mostly applies to low counts), they are corrected strongly and set to `NA` values. If counts are not in the spillover range, then they are left unchanged. Despite setting values to `NA`, correlations between markers are preserved. The marker correlation between HLA-ABC (Yb172Di) and CD3 (Yb173Di) illustrates this point. `CATALYST` removes the positive correlation, whereas `spillR` keeps the correlation for the higher counts. Compensation methods should try to remove spillover while keeping the biological meaningful signal for unbiased downstream analyses. Further experiments on the correlation structure between these markers are necessary to resolve the discrepancy between the two methods. This is an important point as discovering correlations between markers can lead to the discovery of new clusters or signaling networks.

Another advantage of our method is the diagnostic plot of the spillover probability curve. We can judge if the curve makes sense by comparing it to the observed count and bead distributions. Methods based on non-negative least squares are harder to diagnose as they minimize a cost function with no clear biological interpretation.

Currently, we do not take advantage of the target bead distribution in our estimation procedure. We only use spillover bead distributions. In future work, we aim to investigate ways to incorporate the target distribution into our estimation in a nonparametric manner. In our view, one of the biggest strengths of our current method is that it does not assume a specific parametric model for count data. We believe that this is crucial because spillover is just one step that precedes many downstream analysis steps, and avoiding the introduction of bias is thus our top priority.

\textcolor{red}{
Another task for future work is to apply our methodology to imaging mass cytometry (Angeloet al., 2014; Giesenet al., 2014; Bodenmiller, 2016). It will be beneficial to incorporate a spatial regularization term that would enforce neighboring spillover to be similar to one another.
}
<!-- [@angelo2014multiplexed; @giesen2014highly; @bodenmiller2016multiplexed] -->

# Acknowledgments {-}

We thank EuroBioC2022 for awarding Marco Guazzini a travel award to present a preliminary version of `spillR` in Heidelberg. We thank Antoine Chambaz for his feedback on an earlier draft that substantially improved the paper. Alexander G. Reisach received funding from the European Union's Horizon 2020 research and innovation program under the Marie Sk\l{}odowska-Curie grant agreement No 945332 \euflag.

# References {-}

<div id="refs"></div>

\newpage

# (APPENDIX) Supplementary Material {-}

# EM Algorithm Example

Here we illustrate the procedure using a numerical example that includes one target and one spillover marker. We have one data matrix $\mathbf{Y}$ that contains real cell counts recorded for marker 1 (column 1) and the bead counts for marker 1 when the true marker was marker 2 (column 2). In practice, $\mathbf{Y}$ is usually a matrix with more than two columns representing multiple spillover markers. The index $i$ is a specific cell in beads and real cells experiment, respectively. Let's assume the following counts,
$$
\mathbf{Y} = (y_{ij}) = 
\begin{bmatrix}
3 & 2 \\ 
5 & 3 \\ 
17 & 2 \\ 
3   \\ 
17 \\ 
2 
\end{bmatrix}.
$$

```{r}
target    <- c(3, 5, 17,  3,  17, 2)
spillover <- c(2, 3,  2, NA, NA, NA)
Y = dplyr::bind_cols(target = target, spillover = spillover)
Y
```

* Initialization: We initialize our EM algorithm by estimating the conditional probability of observing $y$ given that it belongs to the target marker, and another conditional probability given that it belongs to the spillover marker.

```{r}
y_min <- min(Y$target)
y_max <- max(Y$target)
y_support <- y_min:y_max
fit1 <- density(Y$target, from = y_min, to = y_max)
fit2 <- density(Y$spillover, from = y_min, to = y_max, na.rm = TRUE)
f1 <- approxfun(fit1$x, fit1$y)
f2 <- approxfun(fit2$x, fit2$y)
P_Y1 <- f1(y_support)
P_Y1 <- P_Y1 / sum(P_Y1)
P_Y2 <- f2(y_support)
P_Y2 <- P_Y2 / sum(P_Y2)
P_YZ <- dplyr::bind_cols(P_Y1 = P_Y1, P_Y2 = P_Y2)
```

We initialize the mixture probabilities with the discrete uniform.

```{r}
pi <- c(0.9, 0.1)
```

Now, we update these initial values using the E and M-steps.

* E-step: Calculate the posterior probability for the true marker, and the spillover marker.

```{r}
P_ZY <- dplyr::mutate(P_YZ, 
                      P_Y1 = pi[1] * P_Y1, 
                      P_Y2 = pi[2] * P_Y2)
P_ZY <- P_ZY / rowSums(P_ZY)
P_ZY <- dplyr::bind_cols(target = y_support, P_ZY)
```

* M-step: Update the mixing probability vector,

```{r}
n <- nrow(Y)
YP <- dplyr::left_join(Y, P_ZY, by = "target")
YP
pi <- c(sum(YP$P_Y1) / n, sum(YP$P_Y2) / n)
pi
```

and re-estimate the distribution for the target marker using the posterior probabilities as weights, keep the non-target marker at its initial value,

```{r warning = FALSE}
fit1 <- density(Y$target, from = y_min, to = y_max, weights = YP$P_Y1)
f1 <- approxfun(fit1$x, fit1$y)
P_Y1 <- f1(y_support)
P_Y1 <- P_Y1 / sum(P_Y1)
P_YZ <- bind_cols(P_Y1 = P_Y1, P_Y2 = P_Y2)
```

and calculate the spillover probability estimate,

```{r}
P_ZY <- dplyr::mutate(P_YZ, 
                      P_Y1 = pi[1] * P_Y1, 
                      P_Y2 = pi[2] * P_Y2)
P_ZY <- P_ZY / rowSums(P_ZY)
P_ZY <- dplyr::bind_cols(target = y_support, P_ZY)
P_ZY |>
  dplyr::mutate(p_spillover = round(1 - P_Y1, digits = 3)) |>
  dplyr::select(target, p_spillover) |>
  dplyr::filter(target %in% unique(Y$target))
```

This is the result after one iteration.

# Generative Models

## Bead Shift {-}

Generative model for real cells $Y$ of this experiment:
$$
\begin{aligned}
I              & \sim \text{Bernoulli}(0.1)                            & \qquad \text{(spillover indicator)} \\
Z              & = I + 1                                               & \qquad \text{(channel number)} \\
(Y \mid Z = 1) & \sim \text{Poisson}(200)                              & \qquad \text{(target component)} \\
(Y \mid Z = 2) & \sim \text{Poisson}(70+\tau)                          & \qquad \text{(spillover component with shift)} \\
Y              & = (1-I) \cdot (Y \mid Z = 1) + I \cdot (Y \mid Z = 2) & \qquad \text{(mixture)}.
\end{aligned}
$$
The generative model for beads is an independent copy of the unshifted $Y \mid Z = 2$ at $\tau = 0$.

## Model Misspecification {-}

Generative model for real cells $Y$ of this experiment:
$$
\begin{aligned}
I              & \sim \text{Bernoulli}(0.1)                            & \qquad \text{(spillover indicator)} \\
Z              & = I + 1                                               & \qquad \text{(channel number)} \\
T              & \sim \text{Poisson}(200)                              & \qquad \text{(target)} \\
S              & \sim \text{Poisson}(70)                               & \qquad \text{(spillover)} \\
M              & \sim \text{Bernoulli}(\tau)                           & \qquad \text{(misspecification indicator)} \\
(Y \mid Z = 1) & = (1-M) \cdot T + M \cdot S                           & \qquad \text{(target mixture component)} \\
(Y \mid Z = 2) & = (1-M) \cdot S + M \cdot T                           & \qquad \text{(spillover mixture component)} \\
Y              & = (1-I) \cdot (Y \mid Z = 1) + I \cdot (Y \mid Z = 2) & \qquad \text{(mixture)}
\end{aligned}
$$
The generative model for beads is an independent copy of $Y \mid Z = 2$.

## Bimodal Spillover {-}

Generative model for real cells $Y$ of this experiment:
$$
\begin{aligned}
I               & \sim \text{Bernoulli}(0.1)                            & \qquad \text{(spillover indictor)} \\
Z               & = I + 1                                               & \qquad \text{(channel number)} \\
(Y \mid Z = 1)  & \sim \text{Poisson}(200)                              & \qquad \text{(target component)} \\
H               & \sim \text{Bernoulli}(\tau)                           & \qquad \text{(high count indicator)} \\
(S \mid H = 0)  & \sim \text{Poisson}(70)                               & \qquad \text{(low count component)} \\
(S \mid H = 1)  & \sim \text{Poisson}(330)                              & \qquad \text{(high count component)} \\
( Y \mid Z = 2) & = (1-H) \cdot (S \mid H = 0) + H \cdot (S \mid H = 1) & \qquad \text{(spillover component)} \\
Y               & = (1-I) \cdot (Y \mid Z = 1) + I \cdot (Y \mid Z = 2) & \qquad \text{(mixture)}
\end{aligned}
$$
The generative model for beads is an independent copy of $Y \mid Z = 2$.
