---
title: "`spillR`: Spillover Compensation in Mass Cytometry Data"
author: "Marco Guazzini$^{1}$, Alexander G. Reisach$^{2}$, Sebastian Weichwald$^{3}$, and Christof Seiler$^{1,4,5}$"
date: "$^1$Department of Advanced Computing Sciences, Maastricht University, The Netherlands \\\n $^2$Université Paris Cité, CNRS, MAP5, F-75006 Paris, France \\\n $^3$Department of Mathematical Sciences, University of Copenhagen, Denmark \\\n $^4$Mathematics Centre Maastricht, Maastricht University, The Netherlands \\\n $^5$Center of Experimental Rheumatology, Department of Rheumatology, \\\n University Hospital Zurich, University of Zurich, Switzerland \\\n \\\n `r gsub(' 0', ' ', format(Sys.time(), '%B %d, %Y'))`"
output:
  bookdown::pdf_document2: 
    toc: false
    extra_dependencies: ["euflag"]
bibliography: spillr_paper.bib
csl: style.csl
link-citations: true
abstract: |
  Channel interference in mass cytometry can cause spillover and may result in miscounting of protein markers. @catalyst introduce an experimental and computational procedure to estimate and compensate for spillover implemented in their R package `CATALYST`. They assume spillover can be described by a spillover matrix that encodes the ratio between unstained and stained channels. They estimate the spillover matrix from experiments with beads. We propose to skip the matrix estimation step and work directly with the full bead distributions. We develop a nonparametric finite mixture model and use the mixture components to estimate the probability of spillover. Spillover correction is often a pre-processing step followed by downstream analyses, and choosing a flexible model reduces the chance of introducing biases that can propagate downstream. We implement our method in an R package `spillR` using expectation-maximization to fit the mixture model. We test our method on synthetic and real data from `CATALYST`. We find that our method compensates low counts accurately, does not introduce negative counts, avoids overcompensating high counts, and preserves correlations between markers that may be biologically meaningful.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, echo=FALSE, warning=FALSE, message=FALSE}
library(CATALYST)
library(spillR)
library(nnls)
library(flowCore)
library(ggplot2)
library(tibble)
library(dplyr)
library(readr)
library(tidyr)
library(cowplot)
library(transport)
library(RColorBrewer)
library(spatstat.geom)
library(parallel)
library(kableExtra)
library(hexbin)
library(rbenchmark)
library(latex2exp)
set.seed(23)
tfm <- function(x) asinh(x/5)
```

# Introduction

Mass cytometry makes it possible to count a large number of proteins simultaneously on individual cells [@bandura2009mass; @bendall2011single]. Although mass cytometry has less spillover---measurements from one channel overlap less with those of another---than flow cytometry [@sp-c; @novo2013generalized], spillover is still a problem and affects downstream analyses such as differential testing [@diffcyt; @seiler2021cytoglmm] or dimensionality reduction [@scater]. Reducing spillover by careful design of experiment is possible [@takahashi2017mass], but a purely experimental approach may be neither efficient nor sufficient [@lun2017influence].

@catalyst propose a method for addressing spillover by conducting an experiment on beads. This experiment measures spillover by staining each bead with a single type of antibody. The slope of the regression line between target antibodies and non-target antibodies represents the spillover proportion between channels. @miao2021ab attempt to solve spillover by fitting a mixture model. Our contribution combines the solutions of @catalyst and @miao2021ab. We still require a bead experiment, as in @catalyst, but estimate spillover leveraging a statistical model, as in @miao2021ab. Both previous versions rely on an estimate for the spillover matrix. The spillover matrix encodes the pairwise spillover proportion between channels. We avoid estimating a spillover matrix and instead model spillover by fitting a mixture model to the observed counts. Our main new assumption is that the spillover distribution---not just the spillover proportion---from the bead experiment carries over to the biological experiment. In other words, we transfer the spillover distribution to the real experiment instead of just the spillover proportion encoded in the spillover matrix.

We present our mixture model and link it to calculating spillover probabilities for specific count values In Section \@ref(methods). Our estimation procedure is based on an EM algorithm and logistic regression, and implemented in our new R package `spillR`^[\textcolor{red}{https://bioconductor.org/packages/spillR}]. We conduct experiments on simulated, \textcolor{red}{semi-simulated}, and real data obtained from the `CATALYST` R package [@catalyst] in Section \@ref(results), and discusses our experiments and relates our findings to `CATALYST` in Section \@ref(discussion).

# Methods

\textcolor{red}{
In this section we first illustrate our method \texttt{spillR} (as well as a simple baseline version \texttt{spillR (naive)} by an example, and then describe the algorithm as well as its underlying assumptions. Regarding the terminology, note that in mass cytometry, counts are often referred to as dual counts or signal intensity. We call them counts to emphasize that we rely on the fact that they are non-negative integers as opposed to possibly real valued intensities. 
}

## Example

```{r method-example_comp, echo=FALSE, warning=FALSE, message=FALSE, cache = TRUE}
# constants
bc_key <- c(139, 141:156, 158:176)
ch <- "CD3.2"
ch_metal <- "Yb173Di"
ch_name <- "CD3 (Yb173Di)"
x_lim <- c(0, 7)

# --------- experiment with beads ---------

sce_bead <- prepData(ss_exp)
sce_bead <- assignPrelim(sce_bead, bc_key, verbose = FALSE)
sce_bead <- applyCutoffs(estCutoffs(sce_bead))
sce_bead <- computeSpillmat(sce_bead)

# --------- experiment with real cells ---------

data(mp_cells, package = "CATALYST")
sce <- prepData(mp_cells)

# --------- table for mapping markers and barcode ---------

marker_to_barc <- 
  rowData(sce_bead)[,c("channel_name", "is_bc")] |>
  as_tibble() |>
  dplyr::filter(is_bc == TRUE) |>
  mutate(barcode = bc_key) |>
  select(marker = channel_name, barcode)

# --------- call compensate from compCytof package ---------

sce_spillr <- compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE)
sce_spillr_fast <- compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE, 
                             fast = TRUE)

# --------- run CATALYST ---------

sm <- metadata(sce_bead)$spillover_matrix
sce_catalyst <- CATALYST::compCytof(sce, sm, overwrite = FALSE)

# --------- beads experiment ---------

tb_bead <- metadata(sce_spillr)$beads_distr[[ch_metal]]
tb_bead <- mutate(
  tb_bead, 
  barcode = ifelse(barcode == ch_metal, paste(ch_metal, "(target)"), barcode)
  )

p_beads <- tb_bead |>
  ggplot(aes(tfm(.data[[ch_metal]]), color = barcode)) +
  geom_density(adjust = 1, linewidth = 0.8) +
  xlim(x_lim) +
  xlab(ch_name) +
  ylab("density") + 
  ggtitle("Beads Experiment")

# --------- spillover probability curves ---------

tb_spill_prob <- metadata(sce_spillr)$spillover_est[[ch_metal]]

p_spill <- tb_spill_prob |>
  ggplot(aes(tfm(.data[[ch_metal]]), spill_prob, color="Spillover probability")) +
  geom_line(linewidth = 0.8) +
  scale_color_manual(values = "black") +
  labs(color = "") +
  xlim(x_lim) +
  ylim(c(0, 1)) +
  xlab(ch_name) +
  ylab("probability") +
  ggtitle("Spillover Estimation")

# --------- before and after ---------

exprs_spillr <- sce_spillr |> 
  assay("exprs") |>
  t() |>
  as_tibble() |>
  mutate(correction = "none")
compexprs_spillr <- sce_spillr |> 
  assay("compexprs") |>
  t() |>
  as_tibble() |>
  mutate(correction = "spillR")
compexprs_spillr_fast <- sce_spillr_fast |> 
  assay("compexprs") |>
  t() |>
  as_tibble() |>
  mutate(correction = "spillR (naive)")
compexprs_catalyst <- sce_catalyst |> 
  assay("compexprs") |>
  t() |>
  as_tibble() |>
  mutate(correction = "CATALYST")
combo <- bind_rows(exprs_spillr, compexprs_spillr, 
                   compexprs_spillr_fast, compexprs_catalyst)
combo <- combo |> select(all_of(c(ch, "correction")))
combo$correction <- factor(
  combo$correction, 
  levels = c("none", "spillR", "spillR (naive)", "CATALYST")
  )

p_before_after <- combo |> 
#   filter(.data[[ch]] > 0) |>
  ggplot(aes(.data[[ch]], color = correction, linetype = correction)) + 
  geom_freqpoly(alpha = 1.0, bins = 50, linewidth = 0.8) +
  xlim(x_lim) +
  xlab(ch_name) +
  ggtitle("Spillover Compensation on Real Cells")

```

```{r method-example, fig.height=5.5, fig.width=7, out.width="75%", fig.align="center", fig.cap='\\textcolor{red}{Panel A shows a density plot of target and spillover markers, Panel B shows spillover probability for Yb173Di estimated by \\texttt{spillR}, and Panel C compares spillover compensation by our methods and \\texttt{CATALYST}. Counts are arcsinh transformed with cofactor of five (\\protect\\hyperlink{ref-bendall2011single}{Bendall \\emph{et al.}, 2011}), zero counts are not shown; they are 31 for no compensation, 1603 for \\texttt{spillR}, 2162 for \\texttt{CATALYST}, and 2532 for \\texttt{spillR (naive)}. As shown in Panel C, our baseline method \\texttt{spillR (naive)} performs similarly to \\texttt{CATALYST} and removes the first peak of the uncorrected data (red) between about 2 and 4. By contrast, \\texttt{spillR} is sensitive to the difference in shape between the peaks in the bead data (Panel A) the first peak in the real data (Panec C red), and only removes the part that corresponds to a peak in the bead experiment.}', echo=FALSE, warning=FALSE, message=FALSE, cache = TRUE}
# --------- combine everything ---------
# v <- 2.725
plot_grid(
    p_beads + theme(legend.justification = c(0,1)) + 
        theme(legend.box.margin = margin(-20, 0, 0, 0, unit = "pt")),
    p_spill + theme(legend.justification = c(0,1)) +
        theme(legend.box.margin = margin(-20, 0, 0, 0, unit = "pt")),
    p_before_after + theme(legend.justification = c(0,1)) +
        theme(legend.box.margin = margin(-20, 0, 0, 0, unit = "pt")),
  ncol = 1, align = "v", axis = "lr", labels = c("A", "B", "C"),
  rel_widths = c(1, 1, 1)
)
```

Our procedure is illustrated in Figure \@ref(fig:method-example), using a dataset from the `CATALYST` package as an example. There are four markers, HLA-DR (Yb171Di), HLA-ABC (Yb172Di), CD8 (Yb174Di), and CD45 (Yb176Di), that spill over into the target marker, CD3 (Yb173Di). The markers have two names: the first name is the protein name and the second name in brackets is the conjugated metal. There are bead experiments for each of the spillover markers.

Panel A depicts the marker distributions from the beads experiment. We see that for this marker the bead experiments are high-quality as the target marker Yb173Di is concentrated around six, similarly to the experiment with real cells. This suggests that the spillover marker values can be transferred to the real experiments. Marker Yb172Di shows large spillover into Yb173Di, and suggests that the left tail of the first mode of the distribution may be attributed to that marker. The other spillover markers have low counts, making it justifiable to set some or all the low counts to zero.

\textcolor{red}{Panel B shows a curve representing our spillover probability estimates. We can see that the probability of spillover is highest at points that correspond to a high density of spillover markers in Panel A. If the spillover probability is close to one, our correction step assigns most cells to spillover.} Counts above four stem from spillover with probability zero (and from the actual target with probability one), which means that our procedure keeps them at their raw uncorrected value.

```{r method-example-summary, echo=FALSE, warning=FALSE, message=FALSE}
zeros <- combo |>
  filter(.data[[ch]] == 0 | is.na(.data[[ch]])) |> 
  group_by(correction) |> 
  tally(name = "zeros or \\texttt{NA}'s")

means <- combo |> 
  filter(!is.na(.data[[ch]])) |> 
  group_by(correction) |>
  summarize(mean = mean(.data[[ch]]))

zeros_means <- left_join(zeros, means, by = "correction")
levels(zeros_means$correction)[2] <- "\\texttt{spillR}"
levels(zeros_means$correction)[3] <- "\\texttt{spillR (naive)}"
levels(zeros_means$correction)[4] <- "\\texttt{CATALYST}"

# zeros_means |>
#   kableExtra::kbl(
#     caption = "Additional summaries of the data underlying Figure \\ref{fig:method-example}C.", 
#     booktabs = TRUE, digits = 2, escape = FALSE
#     )
```

Panel C displays the distribution of our target marker, CD3 (Yb173Di) before and after spillover correction. \textcolor{red}{We observe few real counts (red) below a value of $2$, so although all methods perform strong compensation in this range, there is little visible change. From $2$ onward there is a clear distinction between the methods. `CATALYST`, like our baseline `spillR (naive)`, compensates nearly all counts below the second peak of the raw counts (red) as spillover. By contrast, `spillR` compensates only where the relative frequency of the raw counts (red) match the density of spillovermarkers in the bead experiment shown in panel A, and does not remove a part of the first peak as a result. While `CATALYST` shifts the distribution of large counts (around 6) slightly to the left, our methods leave them unaffected as the bead experiment shows no spillover in this region.}
<!-- Zero counts (or equivalently `NA` counts for `spillR`) and mean counts are shown in Table \@ref(tab:method-example-summary). 
As we do not know the true mean in this case, the comparison of means here serves merely as an illustration of the main differences between \texttt{spillR} and \texttt{CATALYST} and not to identify the correct method.
-->
\textcolor{red}{
Our baseline method \texttt{spillR (naive)} is similar to \texttt{CATALYST} in low and medium range, but keeps higher counts unchanged.
}

## Definition of Spillover Probability and Assumptions

We observe a count $Y_i$ of a target marker in cell $i$. We model the observed $Y_i$ as a finite mixture [@mclachlan2019finite] of unobserved true marker counts $Y_i \mid Z_i = 1$ and spillover marker counts $Y_i \mid Z_i = 2, \dots, Y_i \mid Z_i = K$ with mixing probabilities $\pi_{k} = P(Z_i = k)$ for $k = 1, \dots, K$, 
$$
P(Y_i = y) = \sum_{k = 1}^K \pi_k \, P(Y_i = y \mid Z_i = k).
$$
The first mixing probability is the proportion of true signal in the observed counts. The other $K-1$ mixing probabilities are the proportions of spillover. The total sum of mixing probabilities equals one, $\sum_k \pi_k = 1$. The total number of markers in mass cytometry panels is between 30 and 40 [@bendall2011single], but only a small subset of three to four markers spill over into the target marker [@catalyst]. So, typically $K = 1+3$ or $K = 1+4$.

Experimentally, we only measure a sample from the distribution of $Y_i$. The probabilities $\pi_k$ and true distributions $P(Y_i = y \mid Z_i = k)$ are unobserved, and we need to estimate them from data. 
In many applications, the mixture components are \textcolor{red}{modeled to be} in a parametric family, for example, the negative binomial distribution.
As spillover correction is a pre-processing step followed by downstream analyses, choosing the wrong model can introduce biases in the next analysis step. To mitigate such biases, we propose to fit nonparametric mixture components. We make two assumptions that render the components and mixture probabilities identifiable:

* <div id='assumption1'>(A1) Spillover distributions are the same in bead and real experiments.</div> 
The distribution of $Y_i \mid Z_i = k$ for all $k > 1$ is the same in beads and real cells. This assumption allows us to learn the spillover distributions of $Y_i \mid Z_i = k$ for all $k > 1$ from experiments with beads, and transfer them to the experiment with real cells. This assumption relies on high-quality single-stained bead experiments that measure spillover in the same range as the target biological experiment. In other words, a high-quality bead experiment for our method works best if the distribution of bead cells is similar to the distribution of real cells.

* <div id='assumption2'>(A2) For each cell $i$, the observed count $Y_i$ can only be due to one distribution.</div> 
This assumption is already implied by the statement of the mixture model. It allows us to calculate the spillover probability for a given count $Y_i = y$ from the posterior probability that it arises through spillover from markers $k > 1$,
$$
P(\text{spillover} \mid Y_i = y) = P(Z_i > 1 \mid Y_i = y) = 
1 - P(Z_i = 1 \mid Y_i = y) = 
1 - \frac{\pi_1 \, P(Y_i = y \mid Z_i = 1)}{P(Y_i = y)}.
$$
To parse this calculation, recall that in mixture models the $\pi_1$ is the prior probability, $P(Y_i = y \mid Z_i = 1)$ is the conditional probability given the mixture component, and the denominator $P(Y_i = y)$ is the marginal distribution. Applying Bayes rule leads to the posterior probability.

## Estimation of Spillover Probability

We propose a two step procedure for estimating the spillover probability. In step 1, we estimate mixture components and mixture probabilities. We refine these estimates using the EM algorithm [@dempster1977maximum]. In step 2, we use these probability estimates to assign counts to spillover or signal.

We denote the $n \times K$ count matrix as $\mathbf{Y} = (y_{ik})$ with real cells in the first column and beads in columns two and higher. To simplify mathematical notation but without loss of generality, we assume that the number of 
\textcolor{red}{events}
from real and bead experiments have the same $n$. In practice, the number of 
\textcolor{red}{events}
from bead experiments is much smaller than from real experiments. The $k$th column of $\mathbf{Y}$ contains marker counts for the $k$th spillover marker, which represents the empirical spillover distribution of marker $k$ into the target marker, that is, the marker in the first column of $\mathbf{Y}$.

### EM Algorithm

* Initialization: For the mixture probability vector, we assign probability $0.9$ to the the target marker and divide the probability $0.1$ among the spillover markers, 
$$
\hat{\pi}_{1} = 0.9 \text{ and } \hat{\pi}_i = 0.1/(K-1) \text{ for all } i > 1.
$$ 
The procedure is not sensitive to the choice of the initial mixture probability vector. Other settings are possible but may be slower to converge, e.g., setting all probabilities to the same value. Then, we initialize the $k$th mixture component using \textcolor{red}{its probability mass function (PMF) after smoothing and normalizing, $\widehat{P}(Y_i = y \mid Z_i = k)$. We smooth the PMF using kernel density estimation implemented in R function \texttt{density} with the default option for selecting the bandwith of a Gaussian kernel.}

* E-step: We evaluate the posterior probability of a count $y$ belonging to component $k$ (that is, originating from marker $k$),
$$
\widehat{P}\left(Z_i = k \mid Y_i = y \right) = 
\frac
{ \hat{\pi}_k \, \widehat{P}(Y_i = y \mid Z_i = k) }
{ \sum_{k' = 1}^K \hat{\pi}_{k'} \, \widehat{P}(Y_i = y \mid Z_i = k') }.
$$

* M-step: We estimate the new mixture probability vector from posterior probabilities, 
$$
\hat{\pi}_k = 
\frac{1}{n} \sum_{i = 1}^n \widehat{P}\left(Z_i = k \mid Y_i = y \right),
$$
and estimate the new target marker distribution \textcolor{red}{by smoothing and normalizing. Here, we use the R function \texttt{density} again}, but weight each observations according to their posterior probabilities, $\widehat{P} \left(Z_i = 1 \mid Y_i = y \right)$. We only update the target marker, $\widehat{P}(Y_i = y \mid Z_i = 1)$, and keep the other bead distributions, $\widehat{P}(Y_i = y \mid Z_i = k)$ for all $k > 1$, fixed at their initial value.

To refine our estimates, we iterate over the E and M-steps until estimates stabilize. We stop iterating when $\hat{\pi}_1$ changes less than $10^{-5}$ from the previous iteration. The final output is the spillover probability curve with estimates at discrete points in the support of $Y_i$, 
$$
\widehat{P}(\text{spillover} \mid Y_i = y) = 1 - \widehat{P}(Z_i = 1 \mid Y_i = y).
$$

We rely on assumption [(A1)](#assumption1) to justify updating only the distribution of the target marker. We rely on assumption [(A2)](#assumption2) to justify calculating the spillover probability from the mixture model. We refer to Appendix \@ref(em-algorithm-example) for a step-by-step example of our EM algorithm. 

### Spillover Decision

To perform spillover compensation, we draw from a Bernoulli distribution with the spillover probability as parameter to decide whether or not to assign a given count to spillover. \textcolor{red}{We set counts designated to spillover to a user-supplied value. We recommend a value of zero to maintain the overall cellular composition of the sample, or masking them using \texttt{NA} to retain mean values closer to the original distribution.}

<!-- 

## Identifiability

[TODO] Need to think if our model assumptions guarantee identifiability and if our procedure is guaranteed to converge. I think if it is identifiable, then convergence follows from the property of the EM algorithm. Some related work [@hall2003nonparametric; @aragam2020identifiability] that might help. 

-->

## Baseline Method `spillR (naive)`

\textcolor{red}{
We compare our mixture method to a naive baseline method that considers only the bead distributions. Similarly to our standard \texttt{spillR} method, we estimate the bead PMF of each bead $k$ with the kernel density estimator \texttt{density}, $\widehat{P}(Y_i | Z_i = k)$. Then, for all count values $y$ in the range of the bead counts, we separately normalize the PMF at each value $Y_i = y$ and calculate the spillover probability as, 
$$
P(Z_i = k \mid Y_i = y) = 1 - \frac{\widehat{P}(Y_i | Z_i = k)}{\sum_{k'}^K \widehat{P}(Y_i | Z_i = k)}.
$$
The spillover decision is the same as in our standard \texttt{spillR} method.
This is a computationally efficient and simple baseline that, in effect, assigns counts to the marker with the highest density at the count value in the corresponding bead experiment.
}

# Results

We first evaluate our new method `spillR` on simulated datasets. We probe our method to experimentally find its shortcomings. Then, we compare `spillR` to the non-negative least squares method implemented in the R package `CATALYST` on real and \textcolor{red}{semi-simulated} data from the same package. All experiments and plots can be reproduced by compiling the R markdown file `spillR_paper.Rmd`^[https://github.com/ChristofSeiler/spillR_paper].

## Simulated Data

```{r simulated-experiments, echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE}
# --------- global parameters ---------

n_real <- 10000
n_bead <-  1000
lambda_real <- 200
lambda_bead  <- 70
lambda_bead_high  <- 330
spill_prob <- 0.5
n_rep <- 20
n_cores <- 8

# --------- helper functions ---------

compute_average <- function(data) {
  comp <- spillR:::compensate(data$df_real, data$df_bead, 
                              target_marker = "Y", 
                              spillover_markers = "Z")
  y_comp  <- comp$tb_compensate$corrected
  y_truth <- data$Z_target

  tibble(
    mean_obsv = mean(data$df_real$Y),
    mean_comp = mean(y_comp, na.rm = TRUE),
    mean_truth = mean(y_truth)
  )
}

# --------- panel A ---------

generate_data_a <- function(tau) {
  # real experiment
  I    <- rbinom(n = n_real, size = 1, prob = spill_prob)
  Z_target <- rpois(n = n_real, lambda = lambda_real)
  Z_spill  <- rpois(n = n_real, lambda = lambda_bead+tau)
  Y        <- (1-I)*Z_target + I*Z_spill
  df_real  <- tibble(Y = Y, barcode = "Y", type = "real cells")
  
  # bead experiment
  Z_bead   <- rpois(n = n_bead, lambda = lambda_bead)
  df_bead  <- tibble(Y = Z_bead, barcode = "Z", type = "beads")
  
  list(df_real = df_real, df_bead = df_bead, 
       Z_target = Z_target, Z_spill = Z_spill)
}

# create parameter combination
d_a <- expand.grid(
  tau = seq(-10, 0, length.out = 60), 
  replicate = 1:n_rep,
  title = "Bead Shift"
  )
d_averages <- mclapply(d_a$tau, function(tau) {
  data <- generate_data_a(tau)
  compute_average(data)
}, mc.cores = n_cores) |> bind_rows()
d_a <- bind_cols(d_a, d_averages)

# --------- panel B ---------

generate_data_b <- function(tau) {
  # real experiment
  I <- rbinom(n = n_real, size = 1, prob = spill_prob)
  T <- rpois(n = n_real, lambda = lambda_real)
  S <- rpois(n = n_real, lambda = lambda_bead)
  M <- rbinom(n = n_real, size = 1, prob = tau)
  Z_target <- (1-M)*T + M*S
  Z_spill  <- (1-M)*S + M*T
  Y        <- (1-I)*Z_target + I*Z_spill
  df_real  <- tibble(Y = Y,
                     barcode = "Y",
                     type = "real cells")
  
  # bead experiment
  I <- rbinom(n = n_bead, size = 1, prob = spill_prob)
  T <- rpois(n = n_bead, lambda = lambda_real)
  S <- rpois(n = n_bead, lambda = lambda_bead)
  M <- rbinom(n = n_bead, size = 1, prob = tau)
  Z_bead <- (1-M)*S + M*T
  df_bead  <- tibble(Y = Z_bead,
                     barcode = "Z",
                     type = "beads")
  
  list(df_real = df_real, df_bead = df_bead, 
       Z_target = Z_target, Z_spill = Z_spill)
}

# create parameter combination
d_b <- expand.grid(
  tau = seq(from = 0, to = 0.5, length.out = 60), 
  replicate = 1:n_rep,
  title = "Model Misspecification"
  )
d_averages <- mclapply(d_b$tau, function(tau) {
  data <- generate_data_b(tau)
  compute_average(data)
}, mc.cores = n_cores) |> bind_rows()
d_b <- bind_cols(d_b, d_averages)

# --------- panel C ---------

generate_data_c <- function(tau) {
  # real experiment
  I        <- rbinom(n = n_real, size = 1, prob = spill_prob)
  Z_target <- rpois(n = n_real, lambda = lambda_real)
  H        <- rbinom(n = n_real, size = 1, prob = tau)
  Z_spill1 <- rpois(n = n_real, lambda = lambda_bead)
  Z_spill2 <- rpois(n = n_real, lambda = lambda_bead_high)
  Z_spill   <- (1-H)*Z_spill1 + H*Z_spill2
  Y         <- (1-I)*Z_target + I*Z_spill
  df_real   <- tibble(Y = Y, barcode = "Y", type = "real cells")
  
  # bead experiment
  H       <- rbinom(n = n_real, size = 1, prob = tau)
  Z_bead1 <- rpois(n = n_real, lambda = lambda_bead)
  Z_bead2 <- rpois(n = n_real, lambda = lambda_bead_high)
  Z_bead  <- (1-H)*Z_bead1 + H*Z_bead2
  df_bead <- tibble(Y = Z_bead, barcode = "Z", type = "beads")

  list(df_real = df_real, df_bead = df_bead, 
       Z_target = Z_target, Z_spill = Z_spill)
}

# create parameter combination
d_c <- expand.grid(
  tau = seq(from = 0, to = 1, length.out = 60), 
  replicate = 1:n_rep,
  title = "Bimodal Spillover"
  ) 
d_averages <- mclapply(d_c$tau, function(tau) {
  data <- generate_data_c(tau)
  compute_average(data)
}, mc.cores = n_cores) |> bind_rows()
d_c <- bind_cols(d_c, d_averages)

# --------- combined data frame ---------

tb_experiment <- bind_rows(d_a, d_b, d_c)
tb_summary <- tb_experiment |> 
    group_by(title, tau) |> 
    summarize(
      `mean(Y)` = mean(mean_obsv),
      `mean(Y | Z = 1)` = mean(mean_truth),
      `spillR mean(Y)` = mean(mean_comp)
      ) |> 
    ungroup()

tb_summary_long <- pivot_longer(
  tb_summary, -c(title, tau), names_to = "mean", values_to = "count"
  ) |>
  mutate(mean = factor(mean, levels = c("mean(Y)",
                                        "mean(Y | Z = 1)", 
                                        "spillR mean(Y)")))
```

```{r simulated-experiments-critical-values, echo = FALSE, warning = FALSE, message = FALSE}
# --------- helper function ---------
plot_critical <- function(experiment, taus, simulated, reverse = FALSE, custom_tau = "") {
  
  var_names <- c(
    "Y",
    "Y | Z = 1",
    "Y | Z = 2"
    )
  
  simulated <- lapply(simulated, function(data) {
    bind_rows(
      tibble(count = data$df_real$Y, variable = var_names[1]),
      tibble(count = data$Z_target,  variable = var_names[2]),
      tibble(count = data$df_bead$Y, variable = var_names[3])
    )})
  
  df_simulated <- lapply(
    seq(taus), function(i) mutate(simulated[[i]], tau = taus[i])
    ) |> 
    bind_rows() |>
    mutate(variable = factor(variable, levels = var_names)) |>
    mutate(tau = factor(tau, levels = taus))
  
  g_overview <- tb_summary_long |> 
    filter(title == experiment) |>
    ggplot(aes(tau, count, color = mean, linetype = mean)) +
    geom_line() + 
    scale_color_manual(values = c("#E69F00", "#000000", "#009E73")) +
    scale_linetype_manual(values = c("solid", "dashed", "solid")) +
    ylab("mean") + 
    xlab(TeX(paste("$\\tau$", custom_tau))) +
    theme(axis.title.x = element_text(color = "red"))
  if(reverse) g_overview <- g_overview + scale_x_reverse()
  
  g_critical <- df_simulated |>
    ggplot(aes(count, color = variable, linetype = variable)) + 
    geom_density(key_glyph = "path") +
    facet_wrap(~tau, labeller = label_both) +
    scale_color_manual(values = c("#E69F00", "#000000", "#56B4E9")) +
    scale_linetype_manual(values = c("solid", "dashed", "solid"))
  
  plot_grid(
    ggdraw() + 
      draw_label(experiment, x = 0, hjust = 0) + 
      theme(plot.margin = margin(0, 0, 0, 42)), 
    plot_grid(g_overview, g_critical, 
              ncol = 1, rel_heights = c(0.45, 0.55), align = "v", axis = "lr"),
    ncol = 1, rel_heights = c(0.1, 1)
    )
}

# --------- A ---------

experiment <- "Bead Shift"
taus <- c(0, -5, -10)
simulated <- lapply(taus, generate_data_a)
p_a <- plot_critical(experiment, taus, simulated, reverse = TRUE, custom_tau = "(strength of bead distribution shift away from true spillover distribution)")

# --------- B ---------

experiment <- "Model Misspecification"
taus <- c(0, 0.25, 0.5)
simulated <- lapply(taus, generate_data_b)
p_b <- plot_critical(experiment, taus, simulated, custom_tau = "(similarity of target and spillover distribution)")

# --------- C ---------
experiment <- "Bimodal Spillover"
taus <- c(0, 0.5, 1.0)
simulated <- lapply(taus, generate_data_c)
p_c <- plot_critical(experiment, taus, simulated, custom_tau = "(mixing probability of two modes)")
```

```{r simulated-experiments-plot, fig.height = 12, fig.width = 9, out.width = "90%", fig.align = "center", echo = FALSE, warning = FALSE, message = FALSE, fig.cap="Three experiments testing our assumptions and sensitivity to bimodal bead distribution. For each experiment the top row are mean values over the entire range of the experimental setups. \\textcolor{red}{The mean values for \\texttt{spillR} are computed with \\texttt{NA} imputation for spillover counts, so the mean is identical to the true mean without spillover if all spillover counts are correctly identified as such.} The bottom row are density plots for three parameter settings to illustrate the generated distributions. $Y$ is the distribution with spillover. $Y \\mid Z = 1$ is the distribution without spillover. $Y \\mid Z = 2$ is the spillover. mean($Y$) is the average of the distribution with spillover. mean($Y \\mid Z = 1$) is the average count without spillover. \\texttt{spillR} mean($Y$) is the average count after correcting $Y$."}
plot_grid(p_a, p_b, p_c, labels = c("A", "B", "C"),
          ncol = 1, align = "v", axis = "lr")
```

We choose three different experiments to test `spillR` against different bead and real cell distributions. We explore a wide range of possible parameter settings. Figure \@ref(fig:simulated-experiments-plot) has three panels, each representing one experimental setup. The first two panels test our assumptions [(A1)](#assumption1) and [(A2)](#assumption2). The third panel tests sensitivity of `spillR` to bimodal bead distributions. For all three experiments, we model counts using a Poisson distribution with parameter $\lambda$. We simulate 10,000 real cells with $\lambda = 200$, and 1,000 beads with $\lambda = 70$, and a spillover probability of $0.5$. The bead data are an independent copy of the true spillover. The other parameters and statistical dependencies are specific to each experiment. The details of the generative models are given in Appendix \@ref(generative-models). We repeat each simulation 20 times and report averages over the 20 replications. 

Each panel of Figure \@ref(fig:simulated-experiments-plot) has two rows of plots. The plot in the first row represents the summary of the means for each experimental setup as a function of their respective parameter $\tau$. This parameter has a different meaning in each setup. To visualize the different experiments, we summarize the full distributions with the true simulated signal mean (black), the uncorrected mean (orange), and the `spillR` corrected mean (green). Plots on the second row illustrate the simulated data distributions for three selected parameters $\tau$ picked from the experimental setup. The yellow density curve shows the observed counts $Y$. The black density curve shows the distribution of target cell counts. The blue density curve shows the distribution of spillover counts. The goal of the experiment is to estimate the mean of the black density as accurately as possible from the yellow density curve, which represents the data $Y$ we would observe in practice. We simulate this data ourselves with the models in Appendix \@ref(generative-models).

<!--
### Bead Shift (A1)
-->

In the first experiment (panel A), we shift the spillover in the beads experiment away from the true spillover to probe [(A1)](#assumption1). We test a range of bead shifts from no shift at $\tau = 0$ to \textcolor{red}{$\tau = -10$. At $\tau = -10$,} the measured spillover (the first mode of the yellow density) is shifted away from the actual spillover (the blue density), causing both the observed and compensated mean to be lower than the true mean. This may be the case in a low-quality bead experiment. As $\tau$ gets closer to zero, the first mode of the yellow density moves towards the blue density (as may be the case in a higher quality bead experiment), and the compensated signal moves closer to the true mean.
<!-- Our compensation also increases means, as it should, in contrast to e.g. @catalyst. -->

<!--
### Model Misspecification (A2)
-->

In the second experiment (panel B), we mix target and spillover to explore the robustness of our method with respect to our second assumption [(A2)](#assumption2). One way to think about this is that the mixture is a form of model misspecification. Our mixture model is undercomplete, which means that there are more true mixture components than we observe in the beads experiment. If $\tau = 0$, then assumption [(A2)](#assumption2) is correct, but for $\tau = 0.5$ the assumption [(A2)](#assumption2) is maximally violated. The true mean decrease with increasing $\tau$. 
\textcolor{red}{\texttt{spillR} compensates well as long as $\tau$ is close to zero, but deviates from the true mean with increasing $\tau$. As the spillover distribution becomes more similar to the target marker distribution, the mean of the \texttt{spillR} flips to the mean of the observed data at $\tau \approx 0.25$, until}
<!-- Our compensation is closer to the true mean across the tested range. -->
at $\tau = 0.5$ all three distributions and their means are the same.

<!--
### Bimodal Spillover
-->

In the third experiment (panel C), we model spillover with a bimodal distribution. Here $\tau$ is the mixing probability of the two modes. The locations of the two spillover modes are fixed. If $\tau = 0$ or $\tau = 1$, then spillover is unimodal. If $\tau = 0.5$, the first mode of the bimodal beads distribution is left to the signal mode, and the second mode is to the right. The corrected mean is closer to the true mean than the uncorrected mean across the test range.

## Real Data

```{r spillr-vignette, fig.height = 12, fig.width = 10, out.width = "95%", fig.align = "center", echo = FALSE, warning = FALSE, message = FALSE, fig.cap="Comparison of compensation methods and uncorrected counts on real data. Counts are arcsinh transformed with cofactor of five (\\protect\\hyperlink{ref-bendall2011single}{Bendall \\emph{et al.}, 2011}).", cache = TRUE}
# constants
bc_key <- c(139, 141:156, 158:176)

# helper functions
colorscale = scale_fill_gradientn(
  colors = rev(brewer.pal(9, "YlGnBu")),
  values = c(0, exp(seq(-5, 0, length.out = 100)))
  )

# --------- experiment with beads ---------

sce_bead <- prepData(ss_exp)
sce_bead <- assignPrelim(sce_bead, bc_key, verbose = FALSE)
sce_bead <- applyCutoffs(estCutoffs(sce_bead))
sce_bead <- computeSpillmat(sce_bead)

# --------- experiment with real cells ---------

data(mp_cells, package = "CATALYST")
sce <- prepData(mp_cells)

# --------- table for mapping markers and barcode ---------
marker_to_barc <- 
  rowData(sce_bead)[,c("channel_name", "is_bc")] |>
  as_tibble() |>
  dplyr::filter(is_bc == TRUE) |>
  mutate(barcode = bc_key) |>
  dplyr::select(marker = channel_name, barcode)

# --------- call compensate from compCytof package ---------
sce_spillr <- compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE)
sce_spillr_fast <- compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE, 
                             fast = TRUE)

# --------- 2d histogram from spillR (for vignette, not for paper) ---------
# as <- c("counts", "exprs", "compcounts", "compexprs")
# chs <- c( "Yb171Di", "Yb173Di")
# ps <- lapply(as, function(a) 
#     plotScatter(sce_spillr, chs, assay = a))
# plot_grid(plotlist = ps, nrow = 2)

# --------- run CATALYST ---------
sm <- metadata(sce_bead)$spillover_matrix
sce_catalyst <- CATALYST::compCytof(sce, sm, overwrite = FALSE)

# --------- compare spillR and CATALYST (Figure 3B) ---------
exprs_spillr <- sce_spillr |> 
  assay("exprs") |>
  t() |>
  as_tibble() |>
  mutate(method = "uncorrected")
compexprs_spillr <- sce_spillr |> 
  assay("compexprs") |>
  t() |>
  as_tibble() |>
  mutate(method = "spillR")
compexprs_spillr_fast <- sce_spillr_fast |> 
  assay("compexprs") |>
  t() |>
  as_tibble() |>
  mutate(method = "spillR (naive)")
compexprs_catalyst <- sce_catalyst |> 
  assay("compexprs") |>
  t() |>
  as_tibble() |>
  mutate(method = "CATALYST")
combo <- bind_rows(exprs_spillr, compexprs_spillr, 
                   compexprs_spillr_fast, compexprs_catalyst)
combo$method <- factor(
  combo$method, 
  levels = c("uncorrected", "spillR", "spillR (naive)", "CATALYST")
  )

# row 1
p1 <- ggplot(combo, aes(x = CD3.2, y = CD8b)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("CD3 (Yb173Di)") + ylab("CD8 (Yb174Di)")
p2 <- ggplot(combo, aes(x = CD3.2, y = CD8)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("CD3 (Yb173Di)") + ylab("CD8 (La139Di)")

# row 2
p3 <- ggplot(combo, aes(x = HLA.ABC, y = CD3.2)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("CD3 (Yb173Di)")
p4 <- ggplot(combo, aes(x = HLA.ABC, y = CD3.1)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("CD3 (Sm147Di)")

# row 3
p5 <- ggplot(combo, aes(x = HLA.ABC, y = HLA.DR.1)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("HLA-DR (Yb171Di)")
p6 <- ggplot(combo, aes(x = HLA.ABC, y = HLA.DR.2)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("HLA-DR (Lu175Di)")

plot_grid(p1, p2, p3, p4, p5, p6, ncol = 2, 
          labels = c("A", "B", "C", "D", "E", "F"))
```

We compare our methods to `CATALYST` on one of the example datasets in the `CATALYST` package. The dataset consists of an experiment with real cells and corresponding single-stained bead experiments. The experiment on real cells has 5,000 peripheral blood mononuclear cells from healthy donors measured on 39 channels. The experiment on beads has 10,000 cells measured on 36 channels with the number of beads per metal label ranging from 112 to 241.

In Figure \@ref(fig:spillr-vignette), we show the comparison of our methods to `CATALYST` on the same markers as their original paper [@catalyst] in their Figure 3B. In the original experiment, they conjugate the three proteins CD3, CD8, and HLA-DR with two different metal labels. 
\textcolor{red}{
For example, they conjugate CD8 with Yb174Di (Yb is the metal and the number indicates the number of nucleons of the isotope) and La139Di. 
As in their plot, our columns correspond to the different metal labels.
Following their set-up, we show the three target proteins on the vertical axis.
On the horizontal axis we show the spillover markers; we show CD3 in row one, and HLA-ABC in row two and three.
}
We visualize the joint distributions using two-dimensional histograms.

In all six panels (A--F), we observe that `spillR` compensates most strongly in the low counts. In panel C, CD3 (Yb173Di) against HLA-ABC (Yb172Di), `CATALYST` can be seen to compensate strongly in the middle range. It removes the spherical pattern that shows correlation between the two markers. `spillR` preserves this correlation structure and only masks out the lower counts of CD3 (Yb173Di). This highlights a key difference between `spillR` and `CATALYST`: `spillR` \textcolor{identifies counts that may arise from spillover and replaces them with a user-specified value (e.g. 0, \texttt{NA}, or another value), whereas `CATALYST` shrinks counts across the entire range to compensate for spillover.}
\textcolor{red}{
\texttt{spillR (naive)} compensates most aggressively, identifying everything outside the support of the target marker bead distribution as spillover.}


\textcolor{red}{
The panels D and F show no compensation by \texttt{CATALYST}. For the same panels our \texttt{spillR} methods compensate strongly. Our diagnostic plots using function \texttt{plotDiagnostics} in \texttt{spillR} give some indication on why our method compensates that strongly: The original experiment is designed such that we expect no spillover in the channels on the vertical axis, but our diagnostic plots show that the bead distribution of at least one spillover marker---as identified by the spillover matrix in \texttt{CATALYST}---overlap with the first mode in the distribution of real cells. Thus, the strong compensation by \texttt{spillR} is consistent with our assumption (A1). \texttt{spillR (naive)} corroborates our findings as it completely removes all counts in that area, which occurs when there is only spillover and no signal from the target marker.
} 
<!-- [(A1)](#assumption1) -->

The color code of the two-dimensional histograms indicates the absolute number of cells that fall into one hexagon bin. The uncorrected and `spillR` corrected histograms can contain different absolute numbers of cells
\textcolor{red}{
even for identical distributions. The reason for that is that \texttt{spillR} rounds counts to integers and converts spillover counts to \texttt{NA} values. We perform a rounding step to convert raw mass cytometry data that is often not count data to the next lower integer. This often happens because proprietary post-processing of the manufacturer perform a randomization step when exporting the data.
}
The uncorrected counts do not undergo this pre-processing step. `CATALYST` does not perform this pre-processing step. This also explains the different patterns in panel B. `spillR` has horizontal stripes that correspond to non-integer values not in the support of the distribution for `spillR`.
\textcolor{red}{
We leave the decision to apply re-randomization of the count data for downstream analysis up to the user. Our rational is that the user should see the differences in this pre-processing step and how it propagates to the results.
}

```{r computational-resources, echo=FALSE, eval=FALSE}
mark <- benchmark(
  "spillR" = spillR::compCytof(sce, sce_bead, marker_to_barc, n_cores = 8),
  "spillR (naive)" = spillR::compCytof(sce, sce_bead, marker_to_barc, 
                                      fast = TRUE, n_cores = 8),
  "CATALYST" = CATALYST::compCytof(sce, sm),
  replications = 100,
  columns = c("test", "replications", "elapsed")
)
mark$average = mark$elapsed/mark$replications
```

\textcolor{red}{
The average computational time with 100 replications on an Apple M1 with 8 cores and 16 GB of RAM is $10.6$ seconds for \texttt{spillR}, $0.43$ seconds for \texttt{CATALYST}, and $0.45$ seconds for \texttt{spillR (naive)}. The computational costs scale linearly in the number of cells and number of spillover markers. This allows for processing of large scale datasets.
}

## Semi-Simulated Data

```{r semi-simulated-plot, fig.height = 5, fig.width = 10, out.width = "100%", fig.align = "center", echo = FALSE, warning = FALSE, message = FALSE, fig.cap="Comparison of compensation methods and uncorrected counts on semi-synthetic data. The vertical dashed line helps to interpret the spillover correction. It indicates the mode of the bead distribution of Yb173Di at 2.725. Counts are arcsinh transformed with cofactor of five (\\protect\\hyperlink{ref-bendall2011single}{Bendall \\emph{et al.}, 2011}).", cache = TRUE}
# --------- helper function ---------
inv_tfm <- function(x) 5*sinh(x)

# --------- load experiment data of real cells and beads ---------
sce <- prepData(mp_cells)

semi_synthetic_data <- function(shift) {

  # --------- modify bead data ---------
  sce_bead <- prepData(ss_exp)
  
  sce_bead <- assignPrelim(sce_bead, bc_key, verbose = FALSE)
  sce_bead <- applyCutoffs(estCutoffs(sce_bead))
  sce_bead <- computeSpillmat(sce_bead)
  
  counts_bead <- assay(sce_bead, "counts")
  target_channel <- "Yb173Di"
  target_barcode <- "172"
  target_marker <- rowData(sce_bead) |> 
    as_tibble() |>
    filter(channel_name == target_channel) |> 
    pull(marker_name)
  col_ids <- which(sce_bead$bc_id == target_barcode)
  
  counts_real <- assay(sce, "counts")
  y_real <- counts_real[target_marker, ]
  y_bead <- y_real[y_real > 10 & y_real < 300]
  y_bead_shifted <- inv_tfm(tfm(y_bead) - shift)
  y_bead_shifted <- pmax(y_bead_shifted, 0)
  y_bead_small <- sample(y_bead_shifted, length(col_ids))
  counts_bead[target_marker, col_ids] <- y_bead_small
  assay(sce_bead, "counts") <- counts_bead
  
  sce_bead <- assignPrelim(sce_bead, bc_key, verbose = FALSE)
  sce_bead <- applyCutoffs(estCutoffs(sce_bead))
  sce_bead <- computeSpillmat(sce_bead)
  
  # --------- CATALYST ---------
  sm <- metadata(sce_bead)$spillover_matrix
  sce_catalyst <- CATALYST::compCytof(sce, sm, method = "nnls", overwrite = FALSE)
  
  # --------- spillR ---------
  marker_to_barc <- 
    rowData(sce_bead)[,c("channel_name", "is_bc")] |>
    as_tibble() |>
    dplyr::filter(is_bc == TRUE) |>
    mutate(barcode = bc_key) |>
    dplyr::select(marker = channel_name, barcode)
  sce_spillr <- compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE)
  sce_spillr_fast <- compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE, 
                             fast = TRUE)
  
  # --------- beads experiment ---------
  tb_bead <- metadata(sce_spillr)$beads_distr[[ch_metal]]
  tb_bead <- mutate(
    tb_bead, 
    barcode = ifelse(barcode == target_channel, paste(target_channel, "(target)"), barcode)
    )
  tb_bead <- mutate(tb_bead, shift = shift)
  
  # --------- before and after ---------
  exprs_spillr <- sce_spillr |> 
    assay("exprs") |>
    t() |>
    as_tibble() |>
    mutate(correction = "none")
  compexprs_spillr <- sce_spillr |> 
    assay("compexprs") |>
    t() |>
    as_tibble() |>
    mutate(correction = "spillR")
  compexprs_spillr_fast <- sce_spillr_fast |> 
    assay("compexprs") |>
    t() |>
    as_tibble() |>
    mutate(correction = "spillR (naive)")
  compexprs_catalyst <- sce_catalyst |> 
    assay("compexprs") |>
    t() |>
    as_tibble() |>
    mutate(correction = "CATALYST")
  combo <- bind_rows(exprs_spillr, compexprs_spillr, 
                   compexprs_spillr_fast, compexprs_catalyst)
  combo <- combo |> select(all_of(c(ch, "correction")))
  combo$correction <- factor(
    combo$correction, 
    levels = c("none", "spillR", "spillR (naive)", "CATALYST")
    )
  combo <- mutate(combo, shift = shift)
  
  list(tb_bead = tb_bead, combo = combo)

}

semi_list <- lapply(c(0, 0.47, 0.94), semi_synthetic_data)
tb_bead <- lapply(semi_list, function(x) x$tb_bead) |> bind_rows()
combo <- lapply(semi_list, function(x) x$combo) |> bind_rows()

p_beads <- tb_bead |>
  ggplot(aes(tfm(.data[[ch_metal]]), color = barcode)) +
  geom_density(adjust = 1, linewidth = 0.8) +
  xlim(x_lim) +
  xlab(ch_name) +
  ylab("density") + 
  ggtitle("Beads Experiment") +
  facet_wrap(~shift, labeller = label_both)

p_before_after <- combo |> 
  filter(.data[[ch]] > 0) |>
  ggplot(aes(.data[[ch]], color = correction, linetype = correction)) + 
  geom_freqpoly(alpha = 1.0, bins = 50, linewidth = 0.8) +
  xlim(x_lim) +
  xlab(ch_name) +
  ggtitle("Spillover Compensation on Real Cells") +
  facet_wrap(~shift, labeller = label_both)

# --------- combine everything ---------

v <- 2.725
plot_grid(
  p_beads + theme(legend.justification = c(0,1)) + 
    geom_vline(xintercept = v, linetype = "dashed"),
  p_before_after + theme(legend.justification = c(0,1)) +
    geom_vline(xintercept = v, linetype = "dashed"), 
  ncol = 1, align = "v", axis = "lr", labels = c("A", "B")
)
```

\textcolor{red}{
We compare \texttt{spillR} and \texttt{CATALYST} on semi-simulated data. The goal is to elucidate differences between \texttt{spillR} and \texttt{CATALYST}, and to evaluate the performance of \texttt{spillR} if more than one marker spills into the target marker. We create semi-simulated datasets by overwriting the beads distribution for the target marker CD3 (Yb173Di). We consider the first mode of the count distribution of CD3 (Yb173Di) observed in real cells, that is, the counts from $1.44$ to $4.79$ on the transformed scale. We overwrite the beads distribution and shift counts in this range (mostly corresponding to Yb172Di) by three different values: no shift is $0$, subtracting $0.47$ on the transformed scale, and subtracting $0.94$ on the transformed scale. We further subsample without replacement from this new bead distribution to keep the same number of beads as in the original dataset. Figure \ref{fig:semi-simulated-plot} shows the three different beads experiment datasets in row A and the resulting compensations in row B.
}

\textcolor{red}{
In the first column of Figure \ref{fig:semi-simulated-plot}, the bead distributions are equal to the original dataset from Figure \ref{fig:method-example} except Yb172Di is now perfectly aligned with the first mode of the distribution of real cells (red curve in row B). In the second and third column, we shift the bead distribution of Yb172Di by $0.47$ and $0.94$. All three methods correctly compensate the spillover mode when no shift is present (first column). \texttt{CATALYST} and \texttt{spillR(naive)} compensates more aggressively in the medium shift cases (second column), while \texttt{spillR} is more moderate and compensates only the left hand tail of the spillover mode. For a shift of $0.94$ (third column), the three methods differ: \texttt{CATALYST} shrinks counts towards zero, shifting the entire spillover towards zero, \texttt{spillR} compensates lightly on the left hand tail, and \texttt{spillR (naive)} compensates aggressively leaving only a small right hand tail. This experiment illustrates how spillR compensation is strongest for counts that may be attributed to spillover as observed in the beads experiment.
}

# Discussion

The experiment for [(A1)](#assumption1) shows that the mean count after `spillR` correction is closer to the true mean over a wide range of bead shifts. This indicates that our method can perform well even if the bead experiments are imperfect. If the difference between distributions of beads and real cells is large, then one option is to rerun the bead experiments to reduce this gap. The experiment for [(A2)](#assumption2) shows that our method is also robust to model misspecification. Additionally, misspecification can be addressed by adding all channels if necessary. The increase in computational cost when adding channels is relatively minor as our method scales linearly in the number of spillover markers. The experiment on bimodal bead distributions shows that the mean count after correction is still closer to the true mean even with bimodal bead distributions, and also if the spillover is actually larger than the true signal.

In our comparison with `CATALYST` on \textcolor{red}{semi-simulated} and real data, we observe the effect of the two different correction strategies. `CATALYST` essentially shrinks counts towards zero by minimizing a non-negative least squares objective. It assumes that spillover is linear up to counts of 5,000. The applied shrinkage is the same for low counts (e.g., below 10) and high counts (e.g., more than 100). By contrast, `spillR` does not require linearity of the spillover, but assumes that the distribution on the beads experiment carries over to the real cells experiment. In other words, the optimal beads experiment has the same peaks as the real cells experiment.

If counts are in the spillover range (which mostly applies to low counts), they are corrected strongly and set to `NA` values. If counts are not in the spillover range, then they are left unchanged. Despite setting values to `NA`, correlations between markers are preserved. The marker correlation between HLA-ABC (Yb172Di) and CD3 (Yb173Di) illustrates this point. `CATALYST` removes the positive correlation, whereas `spillR` keeps the correlation for the higher counts. Compensation methods should try to remove spillover while keeping the biological meaningful signal for unbiased downstream analyses. Further experiments on the correlation structure between these markers are necessary to resolve the discrepancy between the two methods. This is an important point as discovering correlations between markers can lead to the discovery of new clusters or signaling networks. 

\textcolor{red}{
Our baseline method, \texttt{spillR (naive)}, illustrates the behavior of more aggressive compensation by considering only the bead distribution. If our baseline method compensate aggressively in a certain range, then it means that there are only non-target beads observed in that range.
}

One advantage of our method is the diagnostic plot of the spillover probability curve. We can judge if the curve makes sense by comparing it to the observed count and bead distributions. Methods based on non-negative least squares are harder to diagnose as they minimize a cost function with no clear biological interpretation. In our view, one of the biggest strengths of our current method is that it does not assume a specific parametric model for count data. We believe that this is crucial because spillover is just one step that precedes many downstream analysis steps, and avoiding the introduction of bias is thus our top priority.

\textcolor{red}{
For future work, we plan to apply our methodology to imaging mass cytometry (Angeloet al., 2014; Giesenet al., 2014; Bodenmiller, 2016). The basic method can be applied as is, but it will be beneficial to incorporate a spatial regularization term that enforces neighboring spillover to be similar to one another.
}

# Acknowledgments {-}

We thank EuroBioC2022 for awarding Marco Guazzini a travel award to present a preliminary version of `spillR` in Heidelberg. We thank Antoine Chambaz for his feedback on an earlier draft that substantially improved the paper. Alexander G. Reisach received funding from the European Union's Horizon 2020 research and innovation program under the Marie Sk\l{}odowska-Curie grant agreement No 945332 \euflag.

# References {-}

<div id="refs"></div>

\newpage

# (APPENDIX) Supplementary Material {-}

# EM Algorithm Example

Here we illustrate the procedure using a numerical example that includes one target and one spillover marker. We have one data matrix $\mathbf{Y}$ that contains real cell counts recorded for marker 1 (column 1) and the bead counts for marker 1 when the true marker was marker 2 (column 2). In practice, $\mathbf{Y}$ is usually a matrix with more than two columns representing multiple spillover markers. The index $i$ is a specific cell in beads and real cells experiment, respectively. Let's assume the following counts,
$$
\mathbf{Y} = (y_{ij}) = 
\begin{bmatrix}
3 & 2 \\ 
5 & 3 \\ 
17 & 2 \\ 
3   \\ 
17 \\ 
2 
\end{bmatrix}.
$$

```{r}
target    <- c(3, 5, 17,  3,  17, 2)
spillover <- c(2, 3,  2, NA, NA, NA)
Y = dplyr::bind_cols(target = target, spillover = spillover)
Y
```

* Initialization: We initialize our EM algorithm by estimating the conditional probability of observing $y$ given that it belongs to the target marker, and another conditional probability given that it belongs to the spillover marker.

```{r}
y_min <- min(Y$target)
y_max <- max(Y$target)
y_support <- y_min:y_max
fit1 <- density(Y$target, from = y_min, to = y_max)
fit2 <- density(Y$spillover, from = y_min, to = y_max, na.rm = TRUE)
f1 <- approxfun(fit1$x, fit1$y)
f2 <- approxfun(fit2$x, fit2$y)
P_Y1 <- f1(y_support)
P_Y1 <- P_Y1 / sum(P_Y1)
P_Y2 <- f2(y_support)
P_Y2 <- P_Y2 / sum(P_Y2)
P_YZ <- dplyr::bind_cols(P_Y1 = P_Y1, P_Y2 = P_Y2)
```

We initialize the mixture probabilities with the discrete uniform.

```{r}
pi <- c(0.9, 0.1)
```

Now, we update these initial values using the E and M-steps.

* E-step: Calculate the posterior probability for the true marker, and the spillover marker.

```{r}
P_ZY <- dplyr::mutate(P_YZ, 
                      P_Y1 = pi[1] * P_Y1, 
                      P_Y2 = pi[2] * P_Y2)
P_ZY <- P_ZY / rowSums(P_ZY)
P_ZY <- dplyr::bind_cols(target = y_support, P_ZY)
```

* M-step: Update the mixing probability vector,

```{r}
n <- nrow(Y)
YP <- dplyr::left_join(Y, P_ZY, by = "target")
YP
pi <- c(sum(YP$P_Y1) / n, sum(YP$P_Y2) / n)
pi
```

and re-estimate the distribution for the target marker using the posterior probabilities as weights, keep the non-target marker at its initial value,

```{r warning = FALSE}
fit1 <- density(Y$target, from = y_min, to = y_max, weights = YP$P_Y1)
f1 <- approxfun(fit1$x, fit1$y)
P_Y1 <- f1(y_support)
P_Y1 <- P_Y1 / sum(P_Y1)
P_YZ <- bind_cols(P_Y1 = P_Y1, P_Y2 = P_Y2)
```

and calculate the spillover probability estimate,

```{r}
P_ZY <- dplyr::mutate(P_YZ, 
                      P_Y1 = pi[1] * P_Y1, 
                      P_Y2 = pi[2] * P_Y2)
P_ZY <- P_ZY / rowSums(P_ZY)
P_ZY <- dplyr::bind_cols(target = y_support, P_ZY)
P_ZY |>
  dplyr::mutate(p_spillover = round(1 - P_Y1, digits = 3)) |>
  dplyr::select(target, p_spillover) |>
  dplyr::filter(target %in% unique(Y$target))
```

This is the result after one iteration.

# Generative Models

## Bead Shift {-}

Generative model for real cells $Y$ of this experiment:
$$
\begin{aligned}
I              & \sim \text{Bernoulli}(0.1)                            & \qquad \text{(spillover indicator)} \\
Z              & = I + 1                                               & \qquad \text{(channel number)} \\
(Y \mid Z = 1) & \sim \text{Poisson}(200)                              & \qquad \text{(target component)} \\
(Y \mid Z = 2) & \sim \text{Poisson}(70+\tau)                          & \qquad \text{(spillover component with shift)} \\
Y              & = (1-I) \cdot (Y \mid Z = 1) + I \cdot (Y \mid Z = 2) & \qquad \text{(mixture)}.
\end{aligned}
$$
The generative model for beads is an independent copy of the unshifted $Y \mid Z = 2$ at $\tau = 0$.

## Model Misspecification {-}

Generative model for real cells $Y$ of this experiment:
$$
\begin{aligned}
I              & \sim \text{Bernoulli}(0.1)                            & \qquad \text{(spillover indicator)} \\
Z              & = I + 1                                               & \qquad \text{(channel number)} \\
T              & \sim \text{Poisson}(200)                              & \qquad \text{(target)} \\
S              & \sim \text{Poisson}(70)                               & \qquad \text{(spillover)} \\
M              & \sim \text{Bernoulli}(\tau)                           & \qquad \text{(misspecification indicator)} \\
(Y \mid Z = 1) & = (1-M) \cdot T + M \cdot S                           & \qquad \text{(target mixture component)} \\
(Y \mid Z = 2) & = (1-M) \cdot S + M \cdot T                           & \qquad \text{(spillover mixture component)} \\
Y              & = (1-I) \cdot (Y \mid Z = 1) + I \cdot (Y \mid Z = 2) & \qquad \text{(mixture)}
\end{aligned}
$$
The generative model for beads is an independent copy of $Y \mid Z = 2$.

## Bimodal Spillover {-}

Generative model for real cells $Y$ of this experiment:
$$
\begin{aligned}
I               & \sim \text{Bernoulli}(0.1)                            & \qquad \text{(spillover indictor)} \\
Z               & = I + 1                                               & \qquad \text{(channel number)} \\
(Y \mid Z = 1)  & \sim \text{Poisson}(200)                              & \qquad \text{(target component)} \\
H               & \sim \text{Bernoulli}(\tau)                           & \qquad \text{(high count indicator)} \\
(S \mid H = 0)  & \sim \text{Poisson}(70)                               & \qquad \text{(low count component)} \\
(S \mid H = 1)  & \sim \text{Poisson}(330)                              & \qquad \text{(high count component)} \\
( Y \mid Z = 2) & = (1-H) \cdot (S \mid H = 0) + H \cdot (S \mid H = 1) & \qquad \text{(spillover component)} \\
Y               & = (1-I) \cdot (Y \mid Z = 1) + I \cdot (Y \mid Z = 2) & \qquad \text{(mixture)}
\end{aligned}
$$
The generative model for beads is an independent copy of $Y \mid Z = 2$.
