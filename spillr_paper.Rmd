---
title: "`spillR`: Spillover Compensation in Mass Cytometry Data"
author: "Marco Guazzini$^{1}$, Alexander G. Reisach$^{2}$, Sebastian Weichwald$^{3}$, and Christof Seiler$^{1,4}$"
date: "$^1$Department of Advanced Computing Sciences, Maastricht University, The Netherlands \\\n $^2$Université Paris Cité, CNRS, MAP5, F-75006 Paris, France \\\n $^3$Department of Mathematical Sciences, University of Copenhagen, Denmark \\\n $^4$Mathematics Centre Maastricht, Maastricht University, The Netherlands \\\n \\\n `r gsub(' 0', ' ', format(Sys.time(), '%B %d, %Y'))`"
output:
  bookdown::pdf_document2: 
    toc: false
    keep_tex: true
bibliography: spillr_paper.bib
csl: style.csl
link-citations: true
abstract: |
  Channel interference in mass cytometry can cause spillover and may result in miscounting of protein markers. @catalyst introduce an experimental and computational procedure to estimate and compensate for spillover implemented in their R package `CATALYST`. They assume spillover can be described by a spillover matrix that encodes proportions of true counts over total observed counts. They estimate the spillover matrix from experiments with beads. We propose to skip the matrix estimation step and work directly with the full bead distributions. We develop a nonparametric finite mixture model, and use the mixture components to estimate the probability of spillover. Spillover correction is often a pre-processing step followed by many downstream analyses, choosing a flexible model reduces the chance of introducing biases that can propagaate downstream. We implement our method in an R package `spillR` using expectation-maximization to fit the mixture model. We test our method on synthetic and real data from `CATALYST`. We find that our method compensates low counts accurately, avoids overcompensating high counts, and preserves correlations between markers that may be biologically meaningful.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, echo=FALSE, warning=FALSE, message=FALSE}
library(CATALYST)
library(CytoSpill)
library(nnls)
library(flowCore)
library(ggplot2)
library(tibble)
library(dplyr)
library(magrittr)
library(readr)
library(tidyr)
library(cowplot)
library(transport)
library(RColorBrewer)
library(spatstat.geom)
library(parallel)
source("compensate.R")
source("compCytof.R")
source("plotDiagnostics.R")
set.seed(23)
```

# Introduction

Mass cytometry makes it possible to count a large number of proteins simultaneously on individual cells [@bandura2009mass; @bendall2011single]. Although mass cytometry has less spillover---measurements from one channel overlap less with those of another---than flow cytometry [@sp-c; @novo2013generalized], spillover is still a problem and affects downstream analyses such as differential testing [@diffcyt; @seiler2021cytoglmm] or dimensionality reduction [@scater]. Reducing spillover by careful design of experiment is possible [@takahashi2017mass], but a purely experimental approach may be neither sufficient nor efficient [@lun2017influence].

@catalyst propose a method for addressing spillover by conducting an experiment on beads. This experiment measures spillover by staining each bead with a single type of antibody. The slope of the regression line between target antibody and non-target antibodies represents the spillover proportion between channels. @miao2021ab attempt to solve spillover by fitting a mixture model. Our contribution combines the solutions of @catalyst and @miao2021ab. We still require a bead experiment, as in @catalyst, but estimate spillover leveraging a statistical model, as in @miao2021ab. Both previous versions rely on an estimate for the spillover matrix. The spillover matrix encodes the pairwise spillover proportion between channels. We avoid estimating a spillover matrix and instead model spillover by fitting a mixture model to the observed counts. Our main new assumption is that the spillover distribution---not just the spillover proportion---from the bead experiment carries over to the biological experiment. In other words, we transfer the spillover distribution to the real experiment instead of just the spillover proportion encoded in the spillover matrix.

```{r intro-example, fig.height=3, fig.width=6, out.width="70%", fig.align="center", fig.cap="Two-dimensional histograms of cell counts. The blue shaded rectangle is an estimate of spillover from CD36 (Nd144Di) into CD4 (Nd145Di). Our method compensates for some counts that fall inside this rectangle. Counts are arcsinh transformed with cofactor of five (\\protect\\hyperlink{ref-bendall2011single}{Bendall \\emph{et al.}, 2011}).", echo=FALSE, warning=FALSE, message=FALSE}
# constants
bc_key <- c(139, 141:156, 158:176)

# helper functions
tfm <- function(x) asinh(x/5)
colorscale = scale_fill_gradientn(
  colors = rev(brewer.pal(9, "YlGnBu")),
  values = c(0, exp(seq(-5, 0, length.out = 100)))
  )

# --------- left: experiment with beads ---------

sce_spill <- prepData(ss_exp)
sce_spill <- assignPrelim(sce_spill, bc_key, verbose = FALSE)
sce_spill <- applyCutoffs(estCutoffs(sce_spill))
sce_spill <- computeSpillmat(sce_spill)
counts_spill <- t(assay(sce_spill, "counts"))
counts_spill <- floor(counts_spill)
counts_spill <- as_tibble(counts_spill)

channel_names <- rowData(sce_spill)[,"channel_name"]
names(counts_spill) <- channel_names
counts_spill <- mutate(counts_spill, barcode = sce_spill$bc_id)
counts_spill <- filter(counts_spill, barcode == 144)
counts_spill <- counts_spill %>% select(-barcode)
counts_spill <- mutate(counts_spill, type = "beads for CD36 (Nd144Di)")

# --------- right: experiment with real cells ---------

data(mp_cells, package = "CATALYST")
sce <- prepData(mp_cells)
channel_names <- rowData(sce)[, "channel_name"]
counts_real <- t(assay(sce, "counts"))
counts_real <- floor(counts_real)
colnames(counts_real) <- channel_names
counts_real <- as_tibble(counts_real)
counts_real <- mutate(counts_real, type = "real cells")

# --------- combine tables for plotting ---------
combo <- bind_rows(counts_spill, counts_real)
max_x <- max(counts_spill[,"Nd144Di"])
max_y <- max(counts_spill[,"Nd145Di"])
ggplot(combo, aes(x = tfm(Nd144Di), y = tfm(Nd145Di))) +
  annotate("rect", 
           xmin = 0, xmax = tfm(max_x),
           ymin = 0, ymax = tfm(max_y), 
           alpha = .1, fill = "blue") +
  geom_hex(bins = 32) +
  colorscale +
  facet_wrap(~type) + 
  xlab("CD36 (Nd144Di)") +
  ylab("CD4 (Nd145Di)")
```

To illustrate the main idea of our work, we consider an example with only two markers (Figure \@ref(fig:intro-example)). The left panel depicts an experiment using beads stained only for CD36 (Nd144Di). In this example, the protein CD36 is conjugated with Nd144Di (Nd is the metal and 144 is the number of neutrons of the isotope). Without any spillover, all beads would have zero cell counts for CD4 (ND145Di). Yet, there are many positive CD4 (ND145Di) bead counts. The blue shaded area provides a reasonable estimate of the range of spillover values. The right panel shows an experiment conducted with real cells. Some of the values in the blue shaded area could in principle be real, low counts, but one would expect real counts to be higher and low counts more likely to be due to spillover. While it is not known how many of the low counts are due to spillover, an estimate can be obtained using the bead experiment. We propose to carry over a weighted version of the blue shaded area to the real experiment using finite mixtures. The blue shaded area plays the role of the spillover component and the non-shaded area is the component representing the true marker signal. Our objective is to separate the true signal from the spillover in this mixture. 

This example highlights the main difference between our method and @catalyst. After masking cell marker CD4 (ND145Di) counts that are likely due to spillover, the average of the remaining CD4 (ND145Di) counts will be larger than that of all CD4 (ND145Di) counts. So, in this example, our spillover correction method increases the average count. By contrast, @catalyst will shrink all the counts towards zero. As a consequence, the average count will be lower after correction.

In Section \@ref(methods), we present our mixture model and link it to calculating spillover probabilities for specific count values. Our estimation procedure is based on an EM algorithm and logistic regression, and implemented in our new R package `spillR`^[https://github.com/marcoguazzini/spillR]. In Section \@ref(results), we conduct experiments on simulated and real data obtained from the `CATALYST` R package [@catalyst]. Section \@ref(discussion) discusses our synthetic experiments and relates our findings to `CATALYST`.

# Methods

## Example

```{r method-example, fig.height=7, fig.width=7, out.width="75%", fig.align="center", fig.cap='A: Gaussian kernel density plot of target and spillover markers. B: The solid black curve represents the spillover probability estimated with our method with smoothing parameter $k = 11$. The grey curve is the estimate with $k = 3$. Larger values yield smoother spillover probability curves. C: Frequency polygons of marker CD3 (Yb173Di) comparing no correction, our method, and another method. Zero counts are not plotted. See Table 1 for the zero counts and means. Counts are arcsinh transformed with cofactor of five (\\protect\\hyperlink{ref-bendall2011single}{Bendall \\emph{et al.}, 2011}).', echo=FALSE, warning=FALSE, message=FALSE}
# constants
bc_key <- c(139, 141:156, 158:176)
ch <- "CD3.2"
ch_metal <- "Yb173Di"
ch_name <- "CD3 (Yb173Di)"
runmed_ks <- c(3, 11)
select_k_ind <- which(runmed_ks == 11)
runmed_colors <- c("gray", "gray")
runmed_colors[select_k_ind] <- "black"
x_lim <- c(0, 7)

# --------- experiment with beads ---------

sce_bead <- prepData(ss_exp)
sce_bead <- assignPrelim(sce_bead, bc_key, verbose = FALSE)
sce_bead <- applyCutoffs(estCutoffs(sce_bead))
sce_bead <- computeSpillmat(sce_bead)

# --------- experiment with real cells ---------

data(mp_cells, package = "CATALYST")
sce <- prepData(mp_cells)

# --------- table for mapping markers and barcode ---------

marker_to_barc <- 
  rowData(sce_bead)[,c("channel_name", "is_bc")] %>%
  as_tibble %>%
  dplyr::filter(is_bc == TRUE) %>%
  mutate(barcode = bc_key) %>%
  select(marker = channel_name, barcode)

# --------- call compensate from compCytof package ---------

sce_spillr_list <- lapply(runmed_ks, function(runmed_k) {
  compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE,
            runmed_k = runmed_k)
})

# --------- run CATALYST ---------

sm <- metadata(sce_bead)$spillover_matrix
sce_catalyst <- CATALYST::compCytof(sce, sm, overwrite = FALSE)

# --------- beads experiment ---------

tb_bead <- metadata(sce_spillr_list[[1]])$beads_distr[[ch_metal]]
tb_bead <- mutate(
  tb_bead, 
  barcode = ifelse(barcode == ch, paste(ch, "(target)"), barcode)
  )

p_beads <- tb_bead %>%
  ggplot(aes(tfm(.data[[ch_metal]]), color = barcode)) +
  geom_density(adjust = 1, linewidth = 0.8) +
  xlim(x_lim) +
  xlab(ch_name) +
  ylab("density") + 
  ggtitle("Beads Experiment")

# --------- spillover probability curves ---------

tb_spill_prob <- lapply(seq(runmed_ks), function(i) {
  metadata(sce_spillr_list[[i]])$spillover_est[[ch_metal]] %>%
    mutate(k = as.factor(runmed_ks[i]))
}) %>% bind_rows()

p_spill <- tb_spill_prob %>%
  ggplot(aes(tfm(.data[[ch_metal]]), spill_prob, color = k)) +
  geom_line(linewidth = 0.8) +
  scale_color_manual(values = runmed_colors) +
  xlim(x_lim) +
  xlab(ch_name) +
  ylab("probability") +
  ggtitle("Estimated Spillover")

# --------- before and after ---------

exprs_spillr <- sce_spillr_list[[select_k_ind]] %>% 
  assay("exprs") %>%
  t() %>%
  as_tibble() %>%
  mutate(correction = "none")
compexprs_spillr <- sce_spillr_list[[select_k_ind]] %>% 
  assay("compexprs") %>%
  t() %>%
  as_tibble() %>%
  mutate(correction = "spillR")
compexprs_catalyst <- sce_catalyst %>% 
  assay("compexprs") %>%
  t() %>%
  as_tibble() %>%
  mutate(correction = "other (CATALYST)")
combo <- bind_rows(exprs_spillr, compexprs_spillr, compexprs_catalyst)
combo <- combo %>% select(all_of(c(ch, "correction")))
combo$correction <- factor(combo$correction, 
                       levels = c("none", "spillR", "other (CATALYST)"))

str <- paste0("Spillover Compensation on Real Cells (k = ", 
              runmed_ks[select_k_ind], ")")
p_before_after <- combo %>% 
  filter(.data[[ch]] > 0) %>%
  ggplot(aes(.data[[ch]], color = correction, linetype = correction)) + 
  geom_freqpoly(alpha = 1.0, bins = 50, linewidth = 0.8) +
  xlim(x_lim) +
  xlab(ch_name) +
  ggtitle(str)

# --------- combine everything ---------
plot_grid(
  p_beads + theme(legend.justification = c(0,1)),
  p_spill + theme(legend.justification = c(0,1)), 
  p_before_after + theme(legend.justification = c(0,1)), 
  ncol = 1, align = "v", labels = c("A", "B", "C")
)
```

Figure \@ref(fig:method-example) illustrates our procedure using a dataset from the `CATALYST` package as an example. There are four markers, HLA-DR (Yb171Di), HLA-ABC (Yb172Di), CD8 (Yb174Di), and CD45 (Yb176Di), that spill over into the target marker, CD3 (Yb173Di). The markers have two names: the first name is the protein name and the second name in brackets is the conjugated metal. There are bead experiments for each of the spillover markers.

Panel A depicts the marker distributions from the beads experiment. We see that for this marker the bead experiments are high-quality as the target marker Yb173Di is concentrated around six, similarly to the experiment with real cells. This suggests that the spillover marker values can be transferred to the real experiments. Marker Yb172Di shows large spillover into Yb173Di, and suggests that the left tail of the first mode of the distribution may be attributed to that marker. The other spillover markers have low counts, making it justifiable to set some or all the low counts to zero.

Panel B has a solid black and gray curves representing our spillover probability estimates. With the smoothing parameter $k = 11$, we can see that the probability of spillover goes up to around $0.5$. In that case, our correction step assigns around 50% of cells to spillover, and keeps the other 50% at the current value. Low counts have spillover probability of one, which means that our procedure assigns them to spillover and masks them from the sample by setting them to `NA` values. Counts above four stem from spillover with probability zero (and from the actual target with probability one), which means that our procedure keeps them at their raw uncorrected value. The estimated curve with smoothing parameter $k = 4$ is more irregular, suggesting that these fluctuation may be driven by noise. Users can control the smoothing parameter $k$ to choose the desired bias-variance tradeoff.

```{r method-example-summary, echo=FALSE, warning=FALSE, message=FALSE}
zeros <- combo %>%
  filter(.data[[ch]] == 0 | is.na(.data[[ch]])) %>% 
  group_by(correction) %>% 
  tally(name = "zeros or \\texttt{NA}'s")

means <- combo %>% 
  filter(!is.na(.data[[ch]])) %>% 
  group_by(correction) %>%
  summarize(mean = mean(.data[[ch]]))

zeros_means <- left_join(zeros, means, by = "correction")
levels(zeros_means$correction)[2] <- "\\texttt{spillR}"
levels(zeros_means$correction)[3] <- "other (\\texttt{CATALYST})"

zeros_means %>%
  kableExtra::kbl(
    caption = "Additional summaries of the data underlying Figure 2C.", 
    booktabs = T, digits = 2, escape = FALSE
    )
```

Panel C displays the distribution of our target marker, CD3 (Yb173Di), before and after spillover correction. Our compensation method, `spillR`, masks most markers in the low counts up to two, about 25% to 50% in the medium counts range between two and four, and keeps counts above four. Larger counts are not affected by the correction, as can be seen by the overlap of both curves. In contrast, the other method, `CATALYST`, shifts large counts to the left, and shifts medium counts to low counts or zero counts. Zero counts (or equivalently `NA` counts for `spillR`) and mean counts are shown in Table \@ref(tab:method-example-summary).

## Definition of Spillover Probability and Assumptions

We observe a count $Y_i$ of a target marker in cell $i$. We model the observed $Y_i$ as a finite mixture [@mclachlan2019finite] of unobserved true marker counts $Y_i \mid Z_i = 1$ and spillover marker counts $Y_i \mid Z_i = 2, \dots, Y_i \mid Z_i = K$ with mixing probabilities $\pi_{k} = P(Z_i = k)$ for $k = 1, \dots, K$, 
$$
P(Y_i = y) = \sum_{k = 1}^K \pi_k \, P(Y_i = y \mid Z_i = k).
$$
The first mixing probability is the proportion of true signal in the observed counts. The other $K-1$ mixing probabilities are the proportions of spillover. The total sum of mixing probabilities equals one, $\sum_k \pi_k = 1$. The total number of markers in mass cytometry panels is between 30 and 40 [@bendall2011single], but only a small subset of three to four markers spill over into the target marker [@catalyst]. So, typically $K = 1+3$ or $K = 1+4$.

Experimentally, we only measure a sample from the distribution of $Y_i$. The probabilities $\pi_k$ and true distributions $P(Y_i = y \mid Z_i = k)$ are unobserved, and we need to estimate them from data. In many applications, the mixture components are in a parametric family, for example, the negative binomial distribution. As spillover correction is a pre-processing step followed by downstream analyses, choosing the wrong model can introduce biases in the next analysis step. To mitigate such biases, we propose to fit nonparametric mixture components. We make two assumptions that render the components and mixture probabilities identifiable:

* <div id='assumption1'>(A1) Spillover distributions are the same in bead and real experiments.</div> 
The distribution of $Y_i \mid Z_i = k$ for all $k > 1$ is the same in beads and real cells. This assumption allows us to learn the spillover distributions of $Y_i \mid Z_i = k$ for all $k > 1$ from experiments with beads, and transfer them to the experiment with real cells. This assumption relies on high-quality single stained bead experiments that measure spillover in the same range as the target biological experiment.

* <div id='assumption2'>(A2) For each cell $i$, the observed count $Y_i$ can only be due to one distribution.</div> 
This assumption is already implied by the statement of the mixture model. It allows us to calculate the spillover probability for a given count $Y_i = y$ from the posterior probability that it arises through spillover from markers $k > 1$,
$$
P(\text{spillover} \mid Y_i = y) = P(Z_i > 1 \mid Y_i = y) = 
1 - P(Z_i = 1 \mid Y_i = y) = 
1 - \frac{\pi_1 \, P(Y_i = y \mid Z_i = 1)}{P(Y_i = y)}.
$$
To parse this calculation, recall that in mixture models the $\pi_1$ is the prior probability, $P(Y_i = y \mid Z_i = 1)$ is the conditional probability given the mixture component, and the denominator is the marginal distribution. Putting all this together and applying Bayes rule leads to the posterior probability.

## Estimation of Spillover Probability

We propose a two step procedure for estimating the spillover probability. In step 1, we estimate mixture components and mixture probabilities. We refine these estimates using the EM algorithm [@dempster1977maximum]. In step 2, we use these probability estimates to assign counts to spillover or signal.

We denote the $n \times K$ count matrix as $\mathbf{Y} = (y_{ik})$ with real cells in the first column and beads in columns two and higher. To simplify mathematical notation but without loss of generality, we assume that the number of cells from real and bead experiments have the same $n$. In practice, the number of cells from bead experiments is much smaller than from real experiments. The $k$th column of $\mathbf{Y}$ contains marker counts for the $k$th spillover marker, which represents the empirical spillover distribution of marker $k$ into the target marker, that is, the marker in the first column of $\mathbf{Y}$.

We use the empirical weighted cumulative distribution function (CDF) with non-negative weights $w_i$ at each data point to model the marker distributions $F$ for a specific marker $k$,
$$
\widehat{F}_k(y) = \frac{\sum_{i = 1}^n w_i \, \mathbf{1} \left( y_{ik} \le y \right)}{\sum_{i = 1}^n w_i},
$$
where $\mathbf{1}(\cdot)$ is the indicator function.

### EM Algorithm

* Initialization: For the mixture probability vector, we assign probability $0.9$ to the the target marker and divide the probability $0.1$ among the spillover markers, 
$$
\hat{\pi}_{1} = 0.9 \text{ and } \hat{\pi}_i = 0.1/(K-1) \text{ for all } i > 1,
$$ 
and evaluate the $k$th mixture component using its empirical CDF with equal weights for all data points,
$$
\widehat{P}(Y_i = y \mid Z_i = k) = \widehat{F}_k(y) - \widehat{F}_k(y-1) \text{ with } w_i = 1/n.
$$
We smooth the probability mass function (PMF), $\widehat{P}(Y_i = y \mid Z_i = k)$, using running medians with a windows size of $k$ as implemented in R function `runmed`.

* E-step: We evaluate the posterior probability of a count $y$ belonging to marker $k$,
$$
\widehat{P}\left(Z_i = k \mid Y_i = y \right) = 
\frac
{ \hat{\pi}_k \, \widehat{P}(Y_i = y \mid Z_i = k) }
{ \sum_{k' = 1}^K \hat{\pi}_k' \, \widehat{P}(Y_i = y \mid Z_i = k') }.
$$

* M-step: We estimate the new mixture probability vector from posterior probabilities, 
$$
\hat{\pi}_k = 
\frac{1}{n} \sum_{i = 1}^n \widehat{P}\left(Z_i = k \mid Y_i = y \right),
$$
and estimate the new target marker distribution using its empirical CDF with weights set to posterior probabilities,
$$
\widehat{P}(Y_i = y \mid Z_i = 1) = \widehat{F}_1(y) - \widehat{F}_1(y-1) \text{ with } w_i = \widehat{P} \left(Z_i = 1 \mid Y_i = y \right).
$$
As before, we smooth the PMF, $\widehat{P}(Y_i = y \mid Z_i = 1)$, using running medians with the same windows size of $k$. We keep the bead distributions, $\widehat{P}(Y_i = y \mid Z_i = k)$ for all $k > 1$, fixed at their initial value.

To refine our estimates, we iterate over the E and M-steps until estimates stabilize. We stop iterating when $\hat{\pi}_1$ changes less than $10^{-5}$ from the previous iteration. The final output is the spillover probability curve with estimates at discrete points in the support of $Y_i$, 
$$
\widehat{P}(\text{spillover} \mid Y_i = y) = 1 - \widehat{P}(Z_i = 1 \mid Y_i = y).
$$

We rely on assumption [(A1)](#assumption1) to justify updating only the distribution of the target marker. We rely on assumption [(A2)](#assumption2) to justify calculating the spillover probability from the mixture model. We refer to Appendix \@ref(em-algorithm-example) for a step-by-step example of our EM algorithm. 

### Spillover Decision

To perform the spillover compensation, we draw from a Bernoulli distribution with the spillover probability as parameter to decide whether or not to assign a given count to spillover. We consider spillover counts as having no clear biological interpretation and mask them from our dataset while keeping all other counts. In our implementation, we choose to set spillover counts to `NA` instead of zero to avoid zero-inflated distributions. 

<!-- 

## Identifiability

[TODO] Need to think if our model assumptions guarantee identifiability and if our procedure is guaranteed to converge. I think if it is identifiable, then convergence follows from the property of the EM algorithm. Some related work [@hall2003nonparametric; @aragam2020identifiability] that might help. 

-->

# Results

We first evaluate our new method `spillR` on simulated datasets. We probe our method to experimentally find its shortcomings. Then, we compare `spillR` to the non-negative least squares method implemented in the R package `CATALYST` on real data from the same package. All experiments and plots can be reproduced by compiling the R markdown file `spillR_paper.Rmd`^[https://github.com/ChristofSeiler/spillR_paper].

## Simulated Data

```{r simulated-experiments, echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE}
# --------- global parameters ---------

n_real <-10000
n_bead <- 1000
lambda_real <- 100
lambda_bead  <- 70
lambda_bead_high  <- 130
spill_prob   <- 0.1
n_rep <- 20
runmed_k <- 3
n_cores <- 4

# --------- helper functions ---------

tfm <- function(x) asinh(x/5)

compute_average <- function(data) {
  comp <- compensate(data$df_real, data$df_bead, 
                     target_marker = "Y", 
                     spillover_markers = "Z", 
                     runmed_k = runmed_k)
  y_comp  <- comp$tb_compensate$corrected
  y_truth <- data$Z_target

  tibble(
    mean_obsv = mean(data$df_real$Y),
    mean_comp = mean(y_comp, na.rm = TRUE),
    mean_truth = mean(y_truth)
  )
}

# --------- panel A ---------

generate_data_a <- function(tau) {
  # real experiment
  Z_target <- rpois(n = n_real, lambda = lambda_real)
  Z_spill  <- rpois(n = n_real, lambda = lambda_bead+tau)
  spill    <- rbinom(n = n_real, size = 1, prob = spill_prob)
  Y        <- (1-spill)*Z_target + spill*Z_spill
  df_real  <- tibble(Y = Y, barcode = "Y", type = "real cells")
  
  # bead experiment
  Z_bead   <- rpois(n = n_bead, lambda = lambda_bead)
  df_bead  <- tibble(Y = Z_bead, barcode = "Z", type = "beads")
  
  list(df_real = df_real, df_bead = df_bead, 
       Z_target = Z_target, Z_spill = Z_spill)
}

# create parameter combination
d_a <- expand.grid(
  tau = seq(-30, 0, length.out = 30), 
  replicate = 1:n_rep,
  title = "Bead Shift"
  )
d_averages <- mclapply(d_a$tau, function(tau) {
  data <- generate_data_a(tau)
  compute_average(data)
}, mc.cores = n_cores) %>% bind_rows()
d_a <- bind_cols(d_a, d_averages)

# --------- panel B ---------

generate_data_b <- function(tau) {
  # real experiment
  Z_target <- rpois(n = n_real, lambda = lambda_real)
  Z_spill  <- rpois(n = n_real, lambda = lambda_bead)
  Z_spill_hidden <- rpois(n = n_real, lambda = lambda_bead)
  no_spill <- 1 - sum(spill_prob, tau)
  spill_prob_vec <- c(no_spill, spill_prob, tau)
  spill    <- rmultinom(n = n_real, size = 1, prob = spill_prob_vec)
  Y        <- spill[1, ]*Z_target + spill[2, ]*Z_spill + spill[3, ]*Z_spill_hidden
  df_real  <- tibble(Y = Y,
                     barcode = "Y",
                     type = "real cells")
  
  # bead experiment
  Z_bead   <- rpois(n = n_bead, lambda = lambda_bead)
  df_bead  <- tibble(Y = Z_bead,
                     barcode = "Z",
                     type = "beads")
  
  list(df_real = df_real, df_bead = df_bead, 
       Z_target = Z_target, Z_spill = Z_spill)
}

# create parameter combination
d_b <- expand.grid(
  tau = seq(from = 0, to = 0.1, length.out = 30), 
  replicate = 1:n_rep,
  title = "Model Misspecification"
  )
d_averages <- mclapply(d_b$tau, function(tau) {
  data <- generate_data_b(tau)
  compute_average(data)
}, mc.cores = n_cores) %>% bind_rows()
d_b <- bind_cols(d_b, d_averages)

# --------- panel C ---------

generate_data_c <- function(tau) {
  # real experiment
  Z_target  <- rpois(n = n_real, lambda = lambda_real)
  Z_spill1  <- rpois(n = n_real, lambda = lambda_bead)
  Z_spill2  <- rpois(n = n_real, lambda = lambda_bead_high)
  mode      <- rbinom(n = n_real, size = 1, prob = tau)
  Z_spill   <- (1-mode)*Z_spill1 + mode*Z_spill2
  spill     <- rbinom(n = n_real, size = 1, prob = spill_prob)
  Y         <- (1-spill)*Z_target + spill*Z_spill
  df_real   <- tibble(Y = Y, barcode = "Y", type = "real cells")
  
  # bead experiment
  Z_bead1   <- rpois(n = n_real, lambda = lambda_bead)
  Z_bead2   <- rpois(n = n_real, lambda = lambda_bead_high)
  mode      <- rbinom(n = n_real, size = 1, prob = tau)
  Z_bead    <- (1-mode)*Z_bead1 + mode*Z_bead2
  df_bead   <- tibble(Y = Z_bead, barcode = "Z", type = "beads")

  list(df_real = df_real, df_bead = df_bead, 
       Z_target = Z_target, Z_spill = Z_spill)
}

# create parameter combination
d_c <- expand.grid(
  tau = seq(from = 0, to = 1, length.out = 30), 
  replicate = 1:n_rep,
  title = "Bimodal Spillover"
  ) 
d_averages <- mclapply(d_c$tau, function(tau) {
  data <- generate_data_c(tau)
  compute_average(data)
}, mc.cores = n_cores) %>% bind_rows()
d_c <- bind_cols(d_c, d_averages)

# --------- combined data frame ---------

tb_experiment <- bind_rows(d_a, d_b, d_c)
tb_summary <- tb_experiment %>% 
    group_by(title, tau) %>% 
    summarize(
      `E(Y)` = mean(mean_obsv),
      `E(Y | Z = 1)` = mean(mean_truth),
      `spillR E(Y)` = mean(mean_comp)
      ) %>% 
    ungroup()

tb_summary_long <- pivot_longer(
  tb_summary, -c(title, tau), names_to = "mean", values_to = "count"
  ) %>%
  mutate(mean = factor(mean, levels = c("E(Y)",
                                        "E(Y | Z = 1)", 
                                        "spillR E(Y)")))
```

```{r simulated-experiments-critical-values, echo = FALSE, warning = FALSE, message = FALSE}
# --------- helper function ---------
plot_critical <- function(experiment, tau, simulated) {
  
  var_names <- c(
    "Y",
    "Y | Z = 1",
    "Y | Z = 2"
    )
  
  simulated <- lapply(simulated, function(data) {
    bind_rows(
      tibble(count = data$df_real$Y, variable = var_names[1]),
      tibble(count = data$Z_target,  variable = var_names[2]),
      tibble(count = data$df_bead$Y, variable = var_names[3])
    )})
  
  df_simulated <- lapply(
    seq(tau), function(i) mutate(simulated[[i]], tau = tau[i])
    ) %>% 
    bind_rows() %>%
    mutate(variable = factor(variable, levels = var_names))
  
  g_overview <- tb_summary_long %>% 
    filter(title == experiment) %>%
    ggplot(aes(tau, count, color = mean)) +
    geom_line() + 
    scale_color_manual(values = c("#E69F00", "#000000", "#009E73")) +
    ylab("mean")
  
  g_critical <- df_simulated %>%
    ggplot(aes(count, color = variable)) + 
    geom_density(key_glyph = "path") +
    facet_wrap(~tau, labeller = label_both) +
    scale_color_manual(values = c("#E69F00", "#000000", "#56B4E9", "#0072B2"))
  
  plot_grid(
    ggdraw() + 
      draw_label(experiment, x = 0, hjust = 0) + 
      theme(plot.margin = margin(0, 0, 0, 42)), 
    plot_grid(g_overview, g_critical, 
              ncol = 1, rel_heights = c(0.45, 0.55), align = "v", axis = "lr"),
    ncol = 1, rel_heights = c(0.1, 1)
    )
}

# --------- A ---------

experiment <- "Bead Shift"
tau <- c(-30, -15, 0)
simulated <- lapply(tau, generate_data_a)
p_a <- plot_critical(experiment, tau, simulated)

# --------- B ---------

experiment <- "Model Misspecification"
tau <- c(0, 0.05, 0.1)
simulated <- lapply(tau, generate_data_b)
p_b <- plot_critical(experiment, tau, simulated)

# --------- C ---------
experiment <- "Bimodal Spillover"
tau <- c(0, 0.5, 1.0)
simulated <- lapply(tau, generate_data_c)
p_c <- plot_critical(experiment, tau, simulated)
```

```{r simulated-experiments-plot, fig.height = 12, fig.width = 9, out.width = "90%", fig.align = "center", echo = FALSE, warning = FALSE, message = FALSE, fig.cap="Three experiments testing our assumptions and sensitivy to bimodal bead distribution. For each experiment the top row are mean values over the entire range of the experimental setups, and the bottom row are density plots for three parameter settings to illustrate the generated distributions."}
plot_grid(p_a, p_b, p_c, labels = c("A", "B", "C"),
          ncol = 1, align = "v", axis = "lr")
```

We choose three different experiments to test `spillR` against different bead and real cell distributions. We explore a wide range of possible parameter settings. Figure \@ref(fig:simulated-experiments-plot) has three panels, each representing one experimental setup. The first two panels test our assumptions [(A1)](#assumption1) and [(A2)](#assumption2). The third panel tests sensitivity of `spillR` to bimodal bead distributions. For all three experiments, we model counts using a Poisson distribution with parameter $\lambda$. We simulate 10,000 real cells with $\lambda = 100$, 1,000 beads with $\lambda = 70$, and spillover probability of $0.1$. Beads are an independent copy of the true spillover. The other parameters and statistical dependencies are specific to each experiment. The details of the generative models are given in Appendix \@ref(generative-models). We repeat each simulation 20 times and report averages over the 20 replications. Each panel of Figure \@ref(fig:simulated-experiments-plot) has two rows of plots. The plot on the first row represents the summary of the means for each experimental setup as a function of their respective parameter $\tau$. This parameter has a different meaning in each setup. To visualize the different experiments, we summarize the full distributions with the true simulated signal mean (black), the uncorrected mean (orange), and the `spillR` corrected mean (green). Plots on the second row illustrate the simulated data distributions for three selected parameters $\tau$ picked from the experimental setup. The yellow density curve is the observed count $Y$. The black density curve is the target cell count. The blue density curve is the spillover distribution. The goal of the experiment is to estimate the mean of the black density as accurately as possible from the yellow density curve, which represent the data $Y$ that we would observe in practice. We simulate this data ourselves with the models in Appendix \@ref(generative-models).

<!--
### Bead Shift (A1)
-->

In the first experiment (panel A), we shift the measured beads spillover away from the true bead spillover to probe [(A1)](#assumption1). We test a wide range of bead shifts from $\tau = -30$ to no shift at $\tau = 0$. At $\tau = -30$, the measured spillover---the first mode of the yellow density---is shifted away from the measured spillover---the blue density. Such low-quality beads cause both the observed and compensated mean to be below the true mean. As the beads quality improves, the compensated signal move closer to the true mean. As we increase $\tau$ the first mode of the yellow density moves towards the blue density. This represents high-quality bead experiments.

<!--
### Model Misspecification (A2)
-->

In the second experiment (panel B), we add a hidden spillover marker to explore the robustness of our method with respect to our second assumption [(A2)](#assumption2). One way to think about this is that the hidden spillover is a form of model misspecification. Our mixture model is undercomplete, which means that there are more true mixture components than we observe in the beads experiment. If $\tau = 0$, then assumption [(A2)](#assumption2) is correct. Large $\tau$ violate our assumption. At $\tau = 0.1$, the hidden spillover is as large as the measured spillover. The compensated means are closer to the true means across the test range. The observed mean diverges from the true mean with increasing $\tau$ due to the transfer of mass from the signal to the spillover distribution.

<!--
### High Count Spillover
-->

In the third experiment (panel C), we model spillover with a bimodal distribution. Here $\tau$ is the mixing probability of the two modes. The locations of the two spillover modes are fixed. If $\tau = 0$ or $\tau = 1$, then spillover is unimodal. If $\tau = 0.5$, the first mode of the bimodal beads distribution is left to the signal mode, and the second mode is to the right. The corrected mean is closer to the true mean than the uncorrected mean across the test range. Our method is less sensitive to changes in $\tau$, resulting in less under or over-estimated mean values.

## Real Data

```{r spillr-vignette, fig.height = 6, fig.width = 10, out.width = "100%", fig.align = "center", echo = FALSE, warning = FALSE, message = FALSE, fig.cap="Comparison of compensation methods and uncorrected counts on real data. Counts are arcsinh transformed with cofactor of five (\\protect\\hyperlink{ref-bendall2011single}{Bendall \\emph{et al.}, 2011})."}
 # constants
bc_key <- c(139, 141:156, 158:176)
runmed_k <- 11

# helper functions
colorscale = scale_fill_gradientn(
  colors = rev(brewer.pal(9, "YlGnBu")),
  values = c(0, exp(seq(-5, 0, length.out = 100)))
  )

# --------- experiment with beads ---------

sce_bead <- prepData(ss_exp)
sce_bead <- assignPrelim(sce_bead, bc_key, verbose = FALSE)
sce_bead <- applyCutoffs(estCutoffs(sce_bead))
sce_bead <- computeSpillmat(sce_bead)

# --------- experiment with real cells ---------

data(mp_cells, package = "CATALYST")
sce <- prepData(mp_cells)

# --------- table for mapping markers and barcode ---------
marker_to_barc <- 
  rowData(sce_bead)[,c("channel_name", "is_bc")] %>%
  as_tibble %>%
  dplyr::filter(is_bc == TRUE) %>%
  mutate(barcode = bc_key) %>%
  dplyr::select(marker = channel_name, barcode)

# --------- call compensate from compCytof package ---------
sce_spillr <- compCytof(sce, sce_bead, marker_to_barc, overwrite = FALSE, 
                        runmed_k = runmed_k)

# --------- 2d histogram from spillR (for vignette, not for paper) ---------
# as <- c("counts", "exprs", "compcounts", "compexprs")
# chs <- c( "Yb171Di", "Yb173Di")
# ps <- lapply(as, function(a) 
#     plotScatter(sce_spillr, chs, assay = a))
# plot_grid(plotlist = ps, nrow = 2)

# --------- run CATALYST ---------
sm <- metadata(sce_bead)$spillover_matrix
sce_catalyst <- CATALYST::compCytof(sce, sm, overwrite = FALSE)

# --------- compare spillR and CATALYST (Figure 3B) ---------
exprs_spillr <- sce_spillr %>% 
  assay("exprs") %>%
  t() %>%
  as_tibble() %>%
  mutate(method = "uncorrected")
compexprs_spillr <- sce_spillr %>% 
  assay("compexprs") %>%
  t() %>%
  as_tibble() %>%
  mutate(method = "spillR")
compexprs_catalyst <- sce_catalyst %>% 
  assay("compexprs") %>%
  t() %>%
  as_tibble() %>%
  mutate(method = "CATALYST")
combo <- bind_rows(exprs_spillr, compexprs_spillr, compexprs_catalyst)
combo$method <- factor(combo$method, levels=c('uncorrected', 'CATALYST', 'spillR'))

# row 1
p1 <- ggplot(combo, aes(x = CD3.2, y = CD8b)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("CD3 (Yb173Di)") + ylab("CD8 (Yb174Di)")
p2 <- ggplot(combo, aes(x = CD3.2, y = CD8)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("CD3 (Yb173Di)") + ylab("CD8 (La139Di)")

# row 2
p3 <- ggplot(combo, aes(x = HLA.ABC, y = CD3.2)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("CD3 (Yb173Di)")
p4 <- ggplot(combo, aes(x = HLA.ABC, y = CD3.1)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("CD3 (Sm147Di)")

# row 3
p5 <- ggplot(combo, aes(x = HLA.ABC, y = HLA.DR.1)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("HLA-DR (Yb171Di)")
p6 <- ggplot(combo, aes(x = HLA.ABC, y = HLA.DR.2)) +
  geom_hex(bins = 32) + colorscale + facet_wrap(~method) +
  xlab("HLA-ABC (Yb172Di)") + ylab("HLA-DR (Lu175Di)")

plot_grid(p1, p2, p3, p4, p5, p6, ncol = 2, 
          labels = c("A", "B", "C", "D", "E", "F"))
```

We compare our method to `CATALYST` on one of the example datasets in the `CATALYST` package. The dataset has an experiment with real cells and a corresponding bead experiment. The experiment on real cells has 5,000 peripheral blood mononuclear cells from healthy donors measured on 39 channels. The experiment on beads has 10,000 cells measured on 36 channels. They have single stained bead experiments. The number of beads per metal label range from 112 to 241.

We compare the two methods on the same markers as in the original `CATALYST` paper [@catalyst] in their Figure 3B. In the original experiment, they conjugated three proteins---CD3, CD8, and HLA-DR---with two different metal labels. They conjugated CD8 (first row in Figure \@ref(fig:spillr-vignette)) with Yb174Di (Yb is the metal and 174 is the number of neutrons of the isotope) and La139Di, and similarly for the other rows. On the horizontal axis, we plot the same markers as in the original paper, CD3 and HLA-ABC. We visualize the joint distributions using two-dimensional histograms.

In all six panels (A--F), we observed that `spillR` compensates most strongly in the low counts. In panel C, CD3 (Yb173Di) against HLA-ABC (Yb172Di), `CATALYST` can be seen to compensate strongly in the middle range. It removes the spherical pattern that shows correlation between the two markers. `spillR` preserves this correlation structure and only masks out the lower counts of CD3 (Yb173Di). This highlights a key difference between `spillR` and `CATALYST`: `spillR` does not correct all counts by shrinking them, but rather removes some counts, following the idea that the distribution of the remaining counts is close to the true distribution. `CATALYST` follows another strategy by shrinking counts across the entire range.

The color code of the two-dimensional histograms indicates the absolute number of cells that fall into one hexagon bin. The uncorrected and `spillR` corrected histograms can contain different absolute number of cells because of how `spillR` rounds counts to integers. The raw mass cytometry data is often not count data due to proprietary post-processing of the manufacturer of the mass cytometer. That is why we convert mass cytometry data to count data, we convert the raw values to the next lower integer. The uncorrected counts do not undergo this pre-processing step. 

# Discussion

The experiment for [(A1)](#assumption1) shows that the mean count after `spillR` correction is closer to the true mean over a wide range of bead shifts. This indicates that our method can perform well even if the bead experiments are imperfect. The experiment for [(A2)](#assumption2) shows that our method is also robust to model misspecification. Additionally, misspecification can be addressed by adding all channels if necessary. The increase in computational cost when adding channels is relatively minor as our method scales linearly in the number of spillover markers. The experiment on bimodal bead distributions shows that the mean count after correction is still closer to the true mean even with bimodal bead distributions.

In our comparison to `CATALYST` on real data, we observe the effect of the two different correction strategies. `CATALYST` essentially shrinks counts towards zero by minimizing a non-negative least squares objective. It assumes that spillover is linear up to counts of 5,000. The applied shrinkage is the same for low counts (e.g., below 10) and high counts (e.g., more than 100). By contrast, `spillR` does not require linearity of the spillover, but assumes that the distribution on the beads experiment carries over to the real cells experiment. If counts are in the spillover range (which mostly applies to low counts), they are corrected strongly and set to `NA` values. If counts are not in the spillover range, then they are left unchanged. Despite setting values to `NA`, correlations between markers are preserved. The marker correlation between HLA-ABC (Yb172Di) and CD3 (Yb173Di) illustrates this point. `CATALYST` removes the positive correlation, whereas `spillR` keeps the correlation for the higher counts. Compensation methods should try to remove spillover while keeping biological meaningful signal for unbiased downstream analyses. Another advantage of our method is the diagnostic plot of the spillover probability curve. We can judge if the curve makes sense by comparing it to the observed count and bead distributions. Methods based on non-negative least squares are harder to diagnose as they minimize a cost function with no clear biological interpretation.

Currently, we do not take advantage of the target bead distribution in our estimation procedure. We only use spillover bead distributions. In future work, we aim to investigate ways to incorporate the target distribution into our estimation in a nonparametric manner. In our view, one of the biggest strengths of our current method is that it does not assume a specific parametric model for count data. We believe that this is crucial because spillover is just one step that precedes many downstream analysis steps, and avoiding the introduction of bias is thus our top priority.

# Acknowledgments {-}

We thank EuroBioC2022 for awarding Marco Guazzini a travel award to present a preliminary version of `spillR` in Heidelberg. \textcolor{red}{Alexander, can you check with Antoine if this is OK: We thank Antoine Chambaz for his feedback on an earlier draft that substantially improved our method and the paper}.

# References {-}

<div id="refs"></div>

\newpage

# (APPENDIX) Supplementary Material {-}

# EM Algorithm Example

Here we illustrate the procedure using a numerical example that includes one target and one spillover marker. We have one data matrix $\mathbf{Y}$ that contains real cell counts recorded for marker 1 (column 1) and the bead counts for marker 1 when the true marker was marker 2 (column 2). In practice, $\mathbf{Y}$ is usually a matrix with more than two columns representing multiple spillover markers. The index $i$ is a specific cell in beads and real cells experiment, respectively. Let's assume the following counts,
$$
\mathbf{Y} = (y_{ij}) = 
\begin{bmatrix}
3 & 2 \\ 
5 & 3 \\ 
17 & 2 \\ 
3   \\ 
17 \\ 
2 
\end{bmatrix}.
$$

```{r}
target    <- c(3, 5, 17,  3,  17, 2)
spillover <- c(2, 3,  2, NA, NA, NA)
Y = dplyr::bind_cols(target = target, spillover = spillover)
Y
```

* Initialization: We initialize our EM algorithm by estimating the conditional probability of observing $y$ given that it belongs to the target marker, and another conditional probability given that it belongs to the spillover marker,
$$
\begin{aligned}
& \widehat{P}(Y_i = 2 \mid Z_i = 1)  =  1/6 \qquad & \widehat{P}(Y_i = 2 \mid Z_i = 2) = 2/3 \\
& \widehat{P}(Y_i = 3 \mid Z_i = 1)  =  2/6 \qquad & \widehat{P}(Y_i = 3 \mid Z_i = 2) = 1/3 \\
& \widehat{P}(Y_i = 5 \mid Z_i = 1)  =  1/6 \\
& \widehat{P}(Y_i = 17 \mid Z_i = 1) =  2/6.
\end{aligned}
$$

```{r fig.height=4, fig.width=8, out.width="70%", fig.align="center"}
n1 <- sum(!is.na(Y$target))
n2 <- sum(!is.na(Y$spillover))
F1 <- spatstat.geom::ewcdf(Y$target,    weights = rep(1/n1, n1))
F2 <- spatstat.geom::ewcdf(Y$spillover, weights = rep(1/n2, n2))
par(mfrow = c(1, 2))
plot(F1, main = "CDF of Target",    xlab = "y", ylab = "F1(y)")
plot(F2, main = "CDF of Spillover", xlab = "y", ylab = "F2(y)")
P_Y1 <- c(F1(2)-F1(1), F1(3)-F1(2), F1(5)-F1(4), F1(17)-F1(16))
P_Y2 <- c(F2(2)-F2(1), F2(3)-F2(2), F2(5)-F2(4), F2(17)-F2(16))
smoothing <- function(pmf, k = 1) {
  pmf_smooth <- runmed(pmf, k = k)
  pmf_smooth/sum(pmf_smooth)
}
P_Y1 <- smoothing(P_Y1)
P_Y2 <- smoothing(P_Y2)
P_Y2 <- P_Y2/sum(P_Y2)
P_YZ <- dplyr::bind_cols(P_Y1 = P_Y1, P_Y2 = P_Y2)
P_YZ
```

We initialize the mixture probabilities with the discrete uniform, 
$$
\hat{\pi}_1 = 0.9 \qquad \hat{\pi}_2 = 0.1.
$$ 

```{r}
pi <- c(0.9, 0.1)
```

Now, we update these initial values using the E and M-steps.

* E-step: Calculate the posterior probability for the true marker, and the spillover marker,
$$
\begin{aligned}
& \widehat{P}(Z_i = 1 \mid Y_i = 2)  = \frac{0.9 \cdot 1/6}{0.9 \cdot 1/6 + 0.1 \cdot 2/3} = 0.692 \qquad & \widehat{P}(Z_i = 2 \mid Y_i = 2) = 1 - 0.692 = 0.308 \\
& \widehat{P}(Z_i = 1 \mid Y_i = 3)  = \frac{0.9 \cdot 2/6}{0.9 \cdot 2/6 + 0.1 \cdot 1/3} = 0.9 \qquad & \widehat{P}(Z_i = 2 \mid Y_i = 3) = 1 - 0.9 = 0.1 \\
& \widehat{P}(Z_i = 1 \mid Y_i = 5)  = 1 \\
& \widehat{P}(Z_i = 1 \mid Y_i = 17) = 1.
\end{aligned}
$$

```{r}
P_ZY <- dplyr::mutate(P_YZ, 
                      P_Y1 = pi[1] * P_Y1, 
                      P_Y2 = pi[2] * P_Y2)
P_ZY <- P_ZY / rowSums(P_ZY)
P_ZY <- dplyr::mutate(P_ZY, target = c(2, 3, 5, 17), .before = 1)
P_ZY
```

* M-step: Update the mixing probability vector,
$$
\hat{\pi}_1 = \frac{0.9 + 1 + 1 + 0.9 + 1 + 0.692}{6} = 0.915 \qquad \hat{\pi}_2 = 1 - \hat{\pi}_1 = 0.085,
$$

```{r}
n <- nrow(Y)
YP <- dplyr::left_join(Y, P_ZY, by = "target")
YP
pi <- c(sum(YP$P_Y1)/n, sum(YP$P_Y2)/n)
pi
```

and re-estimate the distribution for the target marker using the posterior probabilities as weights, keep the non-target marker at its initial value, 
$$
\begin{aligned}
& \widehat{P}(Y_i = 2 \mid Z_i = 1)  =  0.126 \qquad & \widehat{P}(Y_i = 2 \mid Z_i = 2) =  2/3 \\
& \widehat{P}(Y_i = 3 \mid Z_i = 1)  =  0.328 & \widehat{P}(Y_i = 3 \mid Z_i = 2) = 1/3 \\
& \widehat{P}(Y_i = 5 \mid Z_i = 1)  =  0.182 & \\
& \widehat{P}(Y_i = 17 \mid Z_i = 1) =  0.364 &
\end{aligned}
$$

```{r fig.height=4, fig.width=8, out.width="70%", fig.align="center"}
F1 <- spatstat.geom::ewcdf(Y$target, weights = YP$P_Y1)
par(mfrow = c(1, 2))
plot(F1, main = "New CDF of Target",    xlab = "y", ylab = "F1(y)")
plot(F2, main = "CDF of Spillover", xlab = "y", ylab = "F2(y)")
P_Y1 <- c(F1(2)-F1(1), F1(3)-F1(2), F1(5)-F1(4), F1(17)-F1(16))
P_Y1 <- smoothing(P_Y1)
P_YZ <- bind_cols(P_Y1 = P_Y1, P_Y2 = P_Y2)
P_YZ
```

and calculate the spillover probability estimate,
$$
\begin{aligned}
& \widehat{P}(\text{spillover} \mid Y_i = 2) = 1 - \frac{0.915 \cdot 0.126}{0.915 \cdot 0.126 + 0.085 \cdot 2/3} = 0.33 \\
& \widehat{P}(\text{spillover} \mid Y_i = 3) = 1 - \frac{0.915 \cdot 0.328}{0.915 \cdot 0.328 + 0.085 \cdot 1/3} = 0.09 \\
& \widehat{P}(\text{spillover} \mid Y_i = 5) = 0 \\
& \widehat{P}(\text{spillover} \mid Y_i = 17) = 0.
\end{aligned}
$$

```{r}
P_ZY <- dplyr::mutate(P_YZ, 
                      P_Y1 = pi[1] * P_Y1, 
                      P_Y2 = pi[2] * P_Y2)
P_ZY <- P_ZY / rowSums(P_ZY)
P_ZY |>
  dplyr::mutate(target = c(2, 3, 5, 17)) |> 
  dplyr::mutate(p_spillover = 1 - P_Y1) |> 
  dplyr::select(target, p_spillover)
```

This is the result after one iteration.

# Generative Models

## Bead Shift {-}

Generative model for real cells $Y$ of this experiment:
$$
\begin{aligned}
S              & \sim \text{Bernoulli}(0.1)   & \qquad \text{(spillover probability } 0.1 \text{)} \\
Z              & = S + 1                      & \qquad \text{(pick spillover)} \\
(Y \mid Z = 1) & \sim \text{Poisson}(100)     & \qquad \text{(target)} \\
(Y \mid Z = 2) & \sim \text{Poisson}(70+\tau) & \qquad \text{(spillover with shift } \tau \text{)}.
\end{aligned}
$$
The generative model for beads is an independent copy of the unshifted $Y \mid Z = 2$ at $\tau = 0$.

## Model Misspecification {-}

Generative model for real cells $Y$ of this experiment:
$$
\begin{aligned}
Z              & \sim \text{Categorical}(1 - (0.1 + \tau), 0.1, \tau) & \qquad \text{(spillover probability } \tau \text{)} \\
(Y \mid Z = 1) & \sim \text{Poisson}(100)                             & \qquad \text{(target)} \\
(Y \mid Z = 2) & \sim \text{Poisson}(70)                              & \qquad \text{(spillover)} \\
(Y \mid Z = 3) & \sim \text{Poisson}(70)                              & \qquad \text{(hidden spillover)}. \\
\end{aligned}
$$
The generative model for beads is an independent copy of $Y \mid Z > 1$.

## High Count Spillover {-}

Generative model for real cells $Y$ of this experiment:
$$
\begin{aligned}
S                & \sim \text{Bernoulli}(0.1)  & \qquad \text{(spillover probability } 0.1 \text{)} \\
Z                & = S + 1                     & \qquad \text{(pick spillover)} \\
(Y \mid Z = 1)   & \sim \text{Poisson}(100)    & \qquad \text{(target)} \\
H                & \sim \text{Bernoulli}(\tau) & \qquad \text{(pick mode with probability } \tau \text{)} \\
(S \mid H = 0)   & \sim \text{Poisson}(70)     & \qquad \text{(spillover)} \\
(S \mid H = 1)   & \sim \text{Poisson}(130)    & \qquad \text{(high count spillover)} \\
( Y \mid Z = 2)  & \overset{d}{=} S            & \qquad \text{(spillover)}.
\end{aligned}
$$
We use $\overset{d}{=}$ to denote that both random variables follow the same distribution. The random variable $S$ is the marginal distribution of the bimodal spillover distribution with high counts. The generative model for beads is an independent copy of $Y \mid Z = 2$.
